---
layout: post
---
<div class="post">

<p>I have learned linear regression a number of times with varying of levels of detail. I'm making this document mostly as a reference for myself and any others who may be interested in the technique.</p>

<p>The concepts and explanations here come either from <a href="http://faculty.marshall.usc.edu/gareth-james/ISL/">An Introduction to Statistical Learning (ISLR)</a> or <a href="https://web.stanford.edu/~hastie/ElemStatLearn/">The Elements of Statistical Learning (ESLR)</a>. Other sources are listed at the bottom.</p>

<figure>
    <img src="/assets/images/linreg.png" alt="Linear Regression">
    <figcaption class="figcaption-text">Good ol' linear regression from <a href="https://en.wikipedia.org/wiki/Linear_regression">Wikipedia</a></figcaption>
</figure>

<p>Note that I will put most everything in multiple linear regression matrix format as it extends to any number of predictors, including the \( y = mx+b \) (i.e. \( y = \beta_1x_1 + \beta_0x_0 \)) classic simple linear regression.</p>

<h2>Notation and Terminology</h2>
<p>
    <ul>
        <p>\(\mathbf{m}\): number of observations in data set <br>(denoted by \(\mathbf{N}\) in ESLR and \(\mathbf{n}\) in ISLR)</p>
        <p>\(\mathbf{n}\): number of predictors in data set <br>(denoted by \(\mathbf{p}\) in both ISLR and ESLR)</p>
        <p><span class="underline">Predictors</span> (also known as features, independent variables, inputs, or explanatory variables): \( x_0 \), \( x_1 \), \( x_2 \)...\( x_n \) (where \( x_0 = 1 \))</p>
        <p><span class="underline">Weights</span> (also known as parameters, multipliers, or regression coefficients): \( \beta_0 \), \( \beta_1 \), \( \beta_2 \)...\( \beta_n \)</p>
        <p><span class="underline">Response</span> (also known as dependent variable): \( y_0 \), \( y_1 \), \( y_2 \)...\( y_m \), the actual data values associated with each set of \( \mathbf{n} \) predictors</p>
        <p><span class="underline">Fitted Values</span> (also known as outputs): \( \hat{y}_0 \), \( \hat{y}_1 \), \( \hat{y}_2 \)...\( \hat{y}_m \), the predictions of the response given some estimated weights \( \hat{\beta}_0 \), \( \hat{\beta}_1 \), \( \hat{\beta}_2 \)...\( \hat{\beta}_n \)</p>
    </ul>
</p>

<!-- Linearity definition -->
<h2>What does <span class="italic">linear</span> mean?</h2>
<p>
    All that <span class="italic">linear</span> means is that the model is of the following form:
    
    \[ f(\mathbf{X}) = \mathbf{X}\beta \]
    
    Here, matrix \(\boldsymbol{X} \in \mathbb{R}^{m \times n}\), weight vector \(\beta \in \mathbb{R}^{n \times 1}\) and the leftmost column of \( \mathbf{X} \) denoted \( X_0 \) is a column of 1s to represent the intercept.
</p>
<p>
    Linear regression is often thought of as a "straight line fit" to a set of observations. It doesn't necessarily result in a straight line or flat plane, though. Consider that \( f(\mathbf{X}) = X_0\beta_0 + X_1\beta_1 + X_1^2\beta_2 + X_1^3\beta_3 \) is still linear, even though it will be look like a cubic fit. Predictors (columns of \( \mathbf{X} \)) can be transformations (e.g. \( X_2 = \log(X_1) \)), basis expansions (e.g. \( X_2 = X_1^2 \) as in the example above), or interactions between other predictors, (e.g. \( X_3 = X_1X_2 \)).
</p>
<p>
    The "true" relationship or "population regression line" between \( Y \in \mathbb{R}^{m \times 1} \) (the response values associated with each row/observation of \( \mathbf{X} \)) and \( \mathbf{X} \) is
    
    \[ Y = f(\mathbf{X}) + \epsilon = \mathbf{X}\beta + \epsilon \]

    Epsilon (\( \epsilon \)) here represents the error term, encompassing omitted and unmeasurable predictors, measurement error of included predictors, and the generic inability of our selected linear model to fit the true relationship.
</p>

<!-- Assumptions -->
<h2>Assumptions of Linear Regression</h2>

<p>After coming up with a linear regression model, there are various required checks in order to validate that the model is acceptable as a description of the data.</p>

<figure>
    <img src="/assets/images/anscombe.png" alt="Anscombe's Quartet">
    <img src="/assets/images/anscombetable.png" alt="Anscombe's Quartet Table">
    <figcaption class="figcaption-text"><a href="https://en.wikipedia.org/wiki/Anscombe%27s_quartet">Anscombe's quartet</a> shows the need for checking assumptions</figcaption>
</figure>
<ol>
    <li>Linearity of the response-predictor relationships.</li>
    <figure>
        <img src="/assets/images/residualvsfit.png" alt="Residuals vs Fitted Vals">
        <figcaption class="figcaption-text">Left plot shows trend in residuals while right plot indicates better fit</figcaption>
    </figure>
    <p>There should be a linear relationship between the predictors and the response. This can be confirmed or denied by plotting the residuals \( e_i = y_i - \hat{y}_i \) versus the fitted values \( \hat{y}_i \). If the residuals look evenly dispersed about the horizontal access, the model is reasonable. Note that a poor residual plot may just be an indicator of poor predictor selection, e.g. non-linear predictors (\( x^2, \sqrt{x} \), etc.) should be fed in to the model.</p>

    <li>No correlation of error terms.</li>
    <figure>
        <img src="/assets/images/correlatederrors.png" alt="Correlated Error Terms">
        <figcaption class="figcaption-text">Correlated errors: adjacent errors tend to be of similar sign and magnitude</figcaption>
    </figure>
    <p>Given ordered data (e.g. time series), plot the residuals in order of time. If patterns arise, it is likely your data has correlated error terms. In time series data, this is also called <a href="https://en.wikipedia.org/wiki/Autocorrelation">autocorrelation</a>. This could occur, for example, if you duplicated your time series dataset - looking at error terms in order, each one would have perfect correlation with the next or previous value! You would get the same parameter fits but drastically different confidence bounds. Other real world examples include sampling biological metrics from the same family or the stock market doing well during certain time periods and poorly in others. At times adding predictors to the model can solve this problem. There are also techniques for removal of autocorrelation with transformation.</p>

    <li>Constant variance of error terms.</li>
    <figure>
        <img src="/assets/images/heteroscedasticity.png" alt="Heteroscedasticity">
        <img src="/assets/images/heteroscedasticity2.png" alt="Heteroscedasticity 2">
        <figcaption class="figcaption-text">Heteroscedasticity can sometimes be removed by transforming the response</figcaption>
    </figure>
    <p>For linear regression's standard errors, confidence bounds, and hypothesis tests, it is assumed that data was generated from a population regression line where the true error \( \epsilon \) is normally distributed with mean 0 and constant variance \( \sigma^2 \), that is \( \epsilon \sim N(0, \sigma^2) \). In reality, it is often the case in that \( \sigma^2 \) varies with the magnitude of the predictors and/or response - this is called heteroscedasticity. Transformation of the response may help the situation. If you have an idea of the variance of error at each response, you can also use weighted least squares.</p>

    <li>No outliers.</li>
    <figure>
        <img src="/assets/images/outlier.png" alt="Outlier">
        <figcaption class="figcaption-text">Spot the outlier</figcaption>
    </figure>
    <p>Outlier points have abnormally high residuals. Methods exist for determining what "abnormal" means here, including examining the studentized residuals (dividing residuals by their estimated standard error). Outliers may not dramatically affect the weights of a model, but can affect the confidence bounds, hypothesis tests and \( R^2 \) value. Removing outliers is sometimes favorable, especially if they stem from something like measurement error. Before removal, it's important to be sure that outliers aren't actually important pieces of data implying that your model is missing predictors or is somehow otherwise deficient.</p>

    <li>No high-leverage points.</li>
    <figure>
        <img src="/assets/images/leverage.png" alt="Leverage">
        <figcaption class="figcaption-text">Leverage: not just for negotiation anymore</figcaption>
    </figure>
    <p>High leverage points have unusual predictor values, e.g. point 41 on the left plot above. Note that in multiple regression it can appear that high leverage points appear normal if you only examine each predictor value individually (middle graph above). Instead, calculate leverage using the diagonal of the "projection matrix" \( \mathbf{H} = \mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T \). Methods for determining what leverage magnitude is unusual enough to be suspect exist, including the <a href="http://www.lithoguru.com/scientist/statistics/Lecture21.pdf">Williams Graph</a> similar the rightmost plot above.</p>

    <li>Collinearity.</li>
    <figure>
        <img src="/assets/images/collinearity.png" alt="Collinearity">
        <figcaption class="figcaption-text">Age and limit are not collinear while rating and limit are highly collinear</figcaption>
    </figure>
    <p>Collinearity occurs when 2 or more predictors are highly related. This reduces the accuracy of the estimates of the weights, as it becomes hard to parse out which predictor is having an effect on the response. Detecting collinearity can be done by calculating the <a href="https://en.wikipedia.org/wiki/Variance_inflation_factor">Variance Inflation Factor</a>, and solved by either removing one of the collinear predictors or combining collinear predictors together.</p>
    
    <li>Endogeneity.</li>
    <p>Endogeneity occurs when there is correlation between model predictor(s) and the true error term \( \epsilon \). Note that the error term here is NOT the estimated error term \( \hat{\epsilon} \) as a result of the fitted values \( \hat{Y} \), but the true error term in the population regression line \( Y = \mathbf{X}\beta + \epsilon \). An example I found explained it well was the burger regression <a href="https://stats.stackexchange.com/questions/263324/how-can-the-regression-error-term-ever-be-correlated-with-the-explanatory-variab">here</a>. Upon suspicion that a predictor is endogenous, a way to test for it is outlined <a href="https://www.youtube.com/watch?v=61I-8tDgwag">here</a>.</p>

    <li>Notes on \( R^2 \) (coefficient of determination)</li>
    <figure>
        <img src="/assets/images/rsquared.png" alt="R squared">
        <figcaption class="figcaption-text">Increasing \( R^2 \) from left to right</figcaption>
    </figure>
    <p>\( R^2 \) does nothing except measure the proportion of variability in Y that can be explained using X. It can always be made closer to 1 by adding more features/variables into the linear model, which at a certain point likely results in a too-flexible (overfit) model that fails to minimize test error. Where \( R^2 \) can be useful is in confirming previously held beliefs about what you are modeling. For example, modeling something in physics that theoretically should be extremely linear, an \( R^2 \) much smaller than 1 indicates something may be off with the model or data. Conversely, a low \( R^2 \) would actually be expected when modeling a complex situation with high residual errors due to other factors.
</ol>

<!-- OLS Closed Form Derivation -->
<h2>Normal Equation for Estimated Weights \( \hat{\beta} \)</h2>
<p>In the Ordinary Least Squares approach, RSS (Residual Sum of Squares) as a function of \( \hat{\beta} \) is the function to minimize:</p>
<figure>
    <img src="/assets/images/residuals.png" alt="Residuals">
    <figcaption class="figcaption-text">Red lines are residuals \( e_i \). RSS is the sum of the square of all residuals.</figcaption>
</figure>
<p>     
    \[ RSS = \sum_{i=1}^{m}e_i^2 = \sum_{i=1}^{m}(y_i - \hat{y}_i)^2  = \sum_{i=1}^{m}(y_i - x_i^T\hat{\beta})^2\]
    where \(y_i\) is the response and \(\hat{y}_i = x_i^T\hat{\beta}\) is the fitted value. This is assuming \(x_i \in \mathbb{R}^{n \times 1}\) and \(\hat{\beta} \in \mathbb{R}^{n \times 1}\) are column vectors and that the first value in each \( x_i \) is 1.
</p>
<p>
    We can rewrite this in matrix form with \(\boldsymbol{X} \in \mathbb{R}^{m \times n}\) and \(y \in \mathbb{R}^{m \times 1}\) as
    \[RSS(\hat{\beta})=(y - \mathbf{X}\hat{\beta})^T(y - \mathbf{X}\hat{\beta})\]

    Each estimated weight vector \(\hat{\beta}\) will give a different \(RSS\) value. Since the premise of Ordinary Least Squares (OLS) is minimizing \(RSS\), we can take the derivative of \(RSS(\hat{\beta})\) and set it to zero to obtain the normal equations for the \(\hat{\beta}\) that minimizes \(RSS\). Note that we use \( (\mathbf{AB})^T = \mathbf{B}^T\mathbf{A}^T \) and the fact that matrices are distributive:
    \[RSS(\hat{\beta})=(y^T - (\mathbf{X}\hat{\beta})^T)(y - \mathbf{X}\hat{\beta})\]
    \[RSS(\hat{\beta})=(y^T - (\hat{\beta}^T\mathbf{X}^T))(y - \mathbf{X}\hat{\beta})\]
    \[RSS(\hat{\beta})=y^Ty - y^T\mathbf{X}\hat{\beta} - (\hat{\beta}^T\mathbf{X}^T)y + \hat{\beta}^T\mathbf{X}^T\mathbf{X}\hat{\beta}\]

    Taking the derivative with respect to the estimated weights:
    \[
    \frac{\partial}{\partial{\hat{\beta}}}RSS(\hat{\beta}) = 
    \color{grey}{\frac{\partial}{\partial{\hat{\beta}}}y^Ty}
    \color{black}{-}
    \color{red}{\frac{\partial}{\partial{\hat{\beta}}}(y^T\mathbf{X}\hat{\beta} + (\hat{\beta}^T\mathbf{X}^T)y)}
    \color{black}{+}
    \color{green}{\frac{\partial}{\partial{\hat{\beta}}}\hat{\beta}^T\mathbf{X}^T\mathbf{X}\hat{\beta}}
    \]

    Now luckily, \(\color{grey}{\frac{\partial}{\partial{\hat{\beta}}}y^Ty}=0\) as response \(y\) is independent of \(\hat{\beta}\). In addition, the red term isn't so bad. Because \(RSS\) is scalar, each term is a scalar. And since scalars are symmetric, \(a^T=a\). Again, we use \( (\mathbf{AB})^T = \mathbf{B}^T\mathbf{A}^T \). So the following holds:
    \[\color{red}{   \frac{\partial}{\partial{\hat{\beta}}} (y^T\mathbf{X}\hat{\beta} + (\hat{\beta}^T\mathbf{X}^T)y) }\]
    \[\color{red}{ = \frac{\partial}{\partial{\hat{\beta}}} ((y^T\mathbf{X}\hat{\beta})^T + (\hat{\beta}^T\mathbf{X}^T)y) }\]
    \[\color{red}{ = \frac{\partial}{\partial{\hat{\beta}}} ((\mathbf{X}\hat{\beta})^Ty + (\hat{\beta}^T\mathbf{X}^T)y) }\]
    \[\color{red}{ = \frac{\partial}{\partial{\hat{\beta}}} ((\hat{\beta}^T\mathbf{X}^T)y + (\hat{\beta}^T\mathbf{X}^T)y) }\]
    \[\color{red}{ = \frac{\partial}{\partial{\hat{\beta}}} 2\hat{\beta}^T\mathbf{X}^Ty }\]

    We can take advantage of the "A is not a function of x" identity <a href="https://en.wikipedia.org/wiki/Matrix_calculus#Vector-by-vector_identities">here</a>, \( \frac{\partial{x^T\mathbf{A}}}{\partial{x}} = \mathbf{A} \). There is a great intuitive walkthrough of this identity <a href="https://eli.thegreenplace.net/2015/the-normal-equation-and-matrix-calculus/#id3">here</a> if interested.
    \[\color{red} {2\frac{\partial\hat{\beta}^T\mathbf{X}^Ty}{\partial{\hat{\beta}}} = 2\mathbf{X}^Ty }\]
    
    The green term uses another identity, "A is not a function of x, A is symmetric" <a href="https://en.wikipedia.org/wiki/Matrix_calculus#Scalar-by-vector_identities">here</a>, \( \frac{\partial{x^T\mathbf{A}x}}{\partial{x}} = 2\mathbf{A}x \). Note that this relies on the \( \mathbf{A} \) matrix being symmetric, but since \( \mathbf{X}^T\mathbf{X} \) is indeed symmetric, this holds. Again, there is a great intuitive walkthrough of this other identity <a href="https://eli.thegreenplace.net/2015/the-normal-equation-and-matrix-calculus/#id3">here</a> if interested as well.
    \[ \color{green} {\frac{\partial\hat{\beta}^T\mathbf{X}^T\mathbf{X}\hat{\beta}}{\partial{\hat{\beta}}} = 2\mathbf{X}^T\mathbf{X}\hat{\beta} }\]

    Finally, we plug in all terms and set to 0:
    \[
    \frac{\partial}{\partial{\hat{\beta}}}RSS(\hat{\beta}) = 
    \color{grey}{0}
    \color{black}{-}
    \color{red}{2\mathbf{X}^Ty}
    \color{black}{+}
    \color{green}{2\mathbf{X}^T\mathbf{X}\hat{\beta}}
    \color{black}{\ = 0}
    \]

    Rearranging, we get:
    \[
    \color{red}{\mathbf{X}^Ty}
    \color{black}{=}
    \color{green}{\mathbf{X}^T\mathbf{X}\hat{\beta}}
    \]

    And if \( \mathbf{X}^T\mathbf{X} \) is nonsingular, i.e. it has an inverse, we can solve directly for \( \hat{\beta} \) using the following:
    \[ \hat{\beta} = (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^Ty\]

    This is called the normal equation. It is helpful when the inverse \( (\mathbf{X}^T\mathbf{X})^{-1}\ \) is not too computationally intensive.
</p>

<p><span class="italic">To be included soon: standard errors, confidence bounds, hypothesis tests, maximum likelihood formulation, numerical methods.</span></p>

<!-- Sources -->
<h2>References:</h2>
<div class="no-bull-list">
    <p><a href="https://eli.thegreenplace.net/2014/derivation-of-the-normal-equation-for-linear-regression/">https://eli.thegreenplace.net/2014/derivation-of-the-normal-equation-for-linear-regression/</a></p>
    <p><a href="https://eli.thegreenplace.net/2015/the-normal-equation-and-matrix-calculus/#id3">https://eli.thegreenplace.net/2015/the-normal-equation-and-matrix-calculus/#id3</a></p>
    <p><a href="https://math.stackexchange.com/questions/2753210/when-can-we-say-that-a-mathrm-t-b-b-mathrm-t-a">https://math.stackexchange.com/questions/2753210/when-can-we-say-that-a-mathrm-t-b-b-mathrm-t-a</a></p>
    <p><a href="https://ayearofai.com/rohan-3-deriving-the-normal-equation-using-matrix-calculus-1a1b16f65dda">https://ayearofai.com/rohan-3-deriving-the-normal-equation-using-matrix-calculus-1a1b16f65dda</a></p>
    <p><a href="https://en.wikipedia.org/wiki/Transpose#Properties">https://en.wikipedia.org/wiki/Transpose#Properties</a></p>
    <p><a href="https://github.com/robinovitch61/coursera_MachineLearning_AndrewNg">https://github.com/robinovitch61/coursera_MachineLearning_AndrewNg</a></p>
    <p><a href="https://www.cs.princeton.edu/courses/archive/fall18/cos324/files/mle-regression.pdf">https://www.cs.princeton.edu/courses/archive/fall18/cos324/files/mle-regression.pdf</a></p>
    <p><a href="https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf">https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf</a></p>
    <p><a href="http://3.droppdf.com/files/pjxkI/regression-analysis-by-example-5th-edition.pdf">http://3.droppdf.com/files/pjxkI/regression-analysis-by-example-5th-edition.pdf</a></p>
    <p><a href="https://stats.stackexchange.com/questions/263324/how-can-the-regression-error-term-ever-be-correlated-with-the-explanatory-variab">https://stats.stackexchange.com/questions/263324/how-can-the-regression-error-term-ever-be-correlated-with-the-explanatory-variab</a></p>
    <p><a href="http://www.lithoguru.com/scientist/statistics/Lecture21.pdf">http://www.lithoguru.com/scientist/statistics/Lecture21.pdf</a></p>
    <p><a href="https://stats.stackexchange.com/questions/349244/beginner-q-residual-sum-squared-rss-and-r2">https://stats.stackexchange.com/questions/349244/beginner-q-residual-sum-squared-rss-and-r2</a></p>
    <p><a href="https://en.wikipedia.org/wiki/Maximum_likelihood_estimation">https://en.wikipedia.org/wiki/Maximum_likelihood_estimation</a></p>
    <p><a href="https://www.youtube.com/watch?v=ulZW99jsMXY">https://www.youtube.com/watch?v=ulZW99jsMXY</a></p>
    <p><a href="https://medium.com/@komotlogelwa/r-squared-and-life-3cb220d5a03f">https://medium.com/@komotlogelwa/r-squared-and-life-3cb220d5a03f</a></p>
    <p><a href=""></a></p>
    <p><a href=""></a></p>
    <p><a href=""></a></p>
    <p><a href=""></a></p>
    <p><a href=""></a></p>
    <p><a href=""></a></p>
    <p><a href=""></a></p>
    <p><a href=""></a></p>
    <p><a href=""></a></p>
    <p><a href=""></a></p>
</div>

</div>