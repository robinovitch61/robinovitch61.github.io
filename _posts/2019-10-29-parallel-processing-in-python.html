---
layout: post
---
<div class="post">

<figure>
    <img style="max-width: 70%" src="/assets/images/concurrency.png" alt="Jigsaw">
    <figcaption class="figcaption-text">Concurrency ! easy is</figcaption>
</figure>

<p>Parallel processing, or concurrency, is of paramount importance in modern computing. It is a complex domain in itself, with major contributions from database development, machine learning, distributed application deployment, and academia.</p>

<p>Python requires a clear and simple concurrency framework as it is commonly used in high-compute settings, like training neural networks. Matrix algebra has been made fast with NumPy and related tools, interfacing with C to get large performance improvements. In the machine learning context, Python often combines concurrency with this efficient toolset to yield readable, easily iterated-upon, rapid results.</p>

<p>Modern computers already use concurrency for almost everything. Processes are just programs given a "time chunk" in which to run on on one of the possibly many cores/CPUs, after which their state is cached efficiently while the next program runs. If a process is running and itself needs to do multiple things (e.g. show a loading icon while also actually loading the content), a process gets split in to threads, which themselves get allocated time chunks from the overall process's time chunk. Processes usually run separate from other processes, with strictly defined ways of interacting with one another, while threads are less rigidly separated from one another.</p>

<p>Underlying the original CPython interpreter is the Global Interpreter Lock (GIL). This ensures that the Python interpreter is controlled by only one thread at a time. This is mainly because Python cleans up memory by "reference counting" instead of other means like garbage collection, ownership, etc. If the reference count for an object drops to zero, Python releases the memory allocated for that object. But multiple threads could change the reference count for an object at the same time, possibly causing memory errors and weird bugs. The GIL is a single lock on the Python interpreter, ensuring that memory is properly managed and no <a href="https://en.wikipedia.org/wiki/Deadlock">deadlocks</a> occur. Because Python was designed decades ago and parallel processing wasn't as high priority, this solution to memory management assumption wasn't seen as a big deal back then.</p>

<p>Unfortunately, this means that multi-threading for CPU-intensive tasks in Python doesn't actually expediate computation - you can write code that implies threads are helping to speed up a high CPU task, but the GIL will ensure that only one thread runs at a time anyway! In fact, multi-threading on high CPU tasks will actually be SLOWER than a single thread, as the GIL has decently high overhead when locking/unlocking threads. The one place multithreading is advantageous is IO operations, as IO operations <a href="https://stackoverflow.com/questions/29270818/why-is-a-python-i-o-bound-task-not-blocked-by-the-gil">do not require the lock from the GIL</a>.</p>

<p>The degredation of performance with threaded compute and improvement in performance with threaded IO is demonstrated in the following static notebook:</p>

<script src="https://gist.github.com/robinovitch61/e9f94dabc8d46b269a279759676596a6.js"></script>

<p>That's all well and good, but what if we need to parallelize a compute-limited operation? That's where the <code>multiprocessing</code> Python module comes in. Processes have more overhead than threads by definition, but get around the GIL-caused limitations of multithreading in CPython, allowing actual compute performance improvements. Additionally, processes don't share global variables the same way threads do. Data sharing between processes is possible but explicitly specified in the code, making them generally safer.</p>

<h2>References:</h2>
<div class="no-bull-list">
    <p><a href="https://realpython.com/python-gil/">https://realpython.com/python-gil/</a></p>
    <p><a href="https://medium.com/practo-engineering/threading-vs-multiprocessing-in-python-7b57f224eadb">https://medium.com/practo-engineering/threading-vs-multiprocessing-in-python-7b57f224eadb</a></p>
    <p><a href="https://wiki.python.org/moin/GlobalInterpreterLock">https://wiki.python.org/moin/GlobalInterpreterLock</a></p>
    <p><a href="https://medium.com/towards-artificial-intelligence/the-why-when-and-how-of-using-python-multi-threading-and-multi-processing-afd1b8a8ecca">https://medium.com/towards-artificial-intelligence/the-why-when-and-how-of-using-python-multi-threading-and-multi-processing-afd1b8a8ecca</a></p>
    <p><a href=""></a></p>
    <p><a href=""></a></p>
    <p><a href=""></a></p>
    <p><a href=""></a></p>
    <p><a href=""></a></p>
    <p><a href=""></a></p>
    <p><a href=""></a></p>
    <p><a href=""></a></p>
    <p><a href=""></a></p>
    <p><a href=""></a></p>
</div>
</div>

<!-- <figure>
    <img style="max-width: 70%" src="/assets/images/jigsaw.png" alt="Jigsaw">
    <figcaption class="figcaption-text">"Eat my spheres!"</figcaption>
</figure>

<p>You find yourself trapped in a room with  Jigsaw, the villain from the Saw movies. There is a wood box of 100 spheres, some of them steel ball bearings and some of them gumballs. Jigsaw tells you that you can leave the room alive if you guess the number of steel ball bearings in the jar correctly. You can pull 10 random spheres out the 100 in the box as many times as you want, but each time you do, you have to eat all of them (yum!). Jigsaw will then replace the stuff you pulled out of the box with new ones - if you pulled 7 ball bearings and 3 gumballs, 7 and 3 are replaced respectively. You're like "cool, I got this, I'm just going to do this a bunch of times and average the number of ball bearings I have to eat each time, then multiply the average by 10 and go home to my family of 12". Little did you know, Jigsaw has implanted magnets in your fingertips, attracting the ball bearings. Your estimate will be biased no matter how many spheres you eat. You will be killed, and your family will mourn.</p>

<p>Bias, both in common vernacular and in statistics, can be thought of as the difference between the average estimate of some truth and the truth itself. Your cousin is biased because his opinions on the evils of gun control, while varied, are on average far from the truth. A statistical model is biased because the average difference between its fitted and true values are non-zero.</p>

<p>Say you estimate a model parameter, e.g. \( \hat{\beta} \), based off a sample of data. Assuming the data comes from some underlying distribution with true parameter \( \beta \), your estimate \( \hat{\beta} \) is unbiased if the average estimate of it is equal to the true parameter, i.e. \( E[\hat{\beta}] = \beta \).</p>

<p>The concept of bias-variance tradeoff is fundamental in modeling. Intuitively, bias is the average difference in model prediction to true value, as discussed above. Variance is how much the model changes when you alter the sample data.</p>

<figure>
    <img src="/assets/images/high_and_low_bias.png" alt="High and Low Bias">
    <figcaption class="figcaption-text">High and low model bias</figcaption>
</figure>

<p>Say, theoretically, that your goal is to have a good model. You might say "I want my model's predictions to be close to reality". Even, "I want the average squared-error of my models predictions to be minimized!". That's not a bad idea! But wait - the image on the left above looks great for that, right? Why is that not the best model? Because the points that this model is going through don't represent all the possible points - it's just a data sample. If you got some more data, your model might not perform very well - its mean-squared error (MSE) might go up.</p>

<p>Demonstrated visually, 3 models fit the data below. They increase in number of predictors, i.e. flexibility of the model. Note how there is always some model error (dashed line on right) - this is the irreducible error, due to things like randomness in the generation of future data and inability to parameterize the universe. The red line represents our test error, our test set being some data we withheld from the model while estimating its parameters. The grey line represents the training error, based on the data we estimated our model parameters with. Training error CAN go to zero, as in the left image above, but we really care about our test error. Test error represents our model's ability to make accurate future predictions.</p>

<figure>
    <img src="/assets/images/bias_variance.png" alt="The bias variance tradeoff">
    <figcaption class="figcaption-text">The bias-variance tradeoff</figcaption>
</figure>

<p>
    Note that MSE can be mathematically decomposed in to bias and variance terms. Assume for some data sample/training set, it came from same "data generating function" \(y = f(x) + \epsilon \) where \( \epsilon \sim N(0, \sigma^2) \). We want to find the \( \hat{f}(x) \) that results in predictions for \( y \) that minimize MSE for data even outside our sample.
    <div style="font-size: 0.9rem !important;">
        \[ E[(y - \hat{f}(x))^2] = MSE \]
        \[ = Bias[\hat{f}(x)]^2 + Var(\hat{f}(x)) + \sigma^2 \]
    </div>

    Proofs for this exist <a href="https://towardsdatascience.com/mse-and-bias-variance-decomposition-77449dd2ff55">here</a> and <a href="https://www.inf.ed.ac.uk/teaching/courses/mlsc/Notes/Lecture4/BiasVariance.pdf">here</a>.
</p>

<p>The bias-variance tradeoff has an important consequence: it is possible that an increase in bias can actually decrease variance to the point where overall MSE is better! A good model minimizes MSE, not just bias or variance.</p>

<p>A concrete example of biased estimators exists below. Generating many samples from an underlying function, both MLE and OLS methods result in unbiased estimates of weights:</p>

<figure>
    <img src="/assets/images/unbiased_weights.png" alt="Unbiased estimated weights">
    <figcaption class="figcaption-text">The mean estimate for each weight is the same as the red true value</figcaption>
</figure>

<p>Importantly, the MLE estimate for error variance \( \sigma^2 \) is biased, whereas the OLS estimate is unbiased. Note that the only difference between the MLE and OLS estimates is dividing by \( m - n \), the number of samples minus the number of predictors. Some intuition for how sample variance is biased without this division can be built from <a href="https://proofwiki.org/wiki/Bias_of_Sample_Variance">this proof</a>, while a more involved proof of the unbiased OLS error variance formula <a href="https://stats.stackexchange.com/questions/20227/why-is-rss-distributed-chi-square-times-n-p">lies here</a>.

<figure>
    <img src="/assets/images/biased_mle_error_var.png" alt="MLE estimate is biased">
    <figcaption class="figcaption-text">MLE (left) mean estimate differs from the true value</figcaption>
</figure>

<p>Complete code for the simulation can be found in <a href="/ols_vs_mle_bias.html">the static notebook here</a>.</p>

<p>I hope this helped develop a deeper understanding of statistical bias. If you have any additions or errors to correct, please open an issue on the source of this website <a href="https://github.com/robinovitch61/robinovitch61.github.io">here.</a></p>

<!-- Sources -->
<h2>References:</h2>
<div class="no-bull-list">
    <p><a href="https://towardsdatascience.com/understanding-the-bias-variance-tradeoff-165e6942b229">https://towardsdatascience.com/understanding-the-bias-variance-tradeoff-165e6942b229</a></p>
    <p><a href="http://faculty.marshall.usc.edu/gareth-james/ISL/">http://faculty.marshall.usc.edu/gareth-james/ISL/</a></p>
    <p><a href="https://www.youtube.com/watch?v=C3nIFH649wY">https://www.youtube.com/watch?v=C3nIFH649wY</a></p>
    <p><a href="https://stats.stackexchange.com/questions/207760/when-is-a-biased-estimator-preferable-to-unbiased-one">https://stats.stackexchange.com/questions/207760/when-is-a-biased-estimator-preferable-to-unbiased-one</a></p>
    <p><a href="https://en.wikipedia.org/wiki/Bias–variance_tradeof">https://en.wikipedia.org/wiki/Bias–variance_tradeof</a></p>
    <p><a href="https://proofwiki.org/wiki/Bias_of_Sample_Variance">https://proofwiki.org/wiki/Bias_of_Sample_Variance</a></p>
    <p><a href="https://www.inf.ed.ac.uk/teaching/courses/mlsc/Notes/Lecture4/BiasVariance.pdf">https://www.inf.ed.ac.uk/teaching/courses/mlsc/Notes/Lecture4/BiasVariance.pdf</a></p>
    <p><a href="https://towardsdatascience.com/mse-and-bias-variance-decomposition-77449dd2ff55">https://towardsdatascience.com/mse-and-bias-variance-decomposition-77449dd2ff55</a></p>
    <p><a href=""></a></p>
    <p><a href=""></a></p>
    <p><a href=""></a></p>
    <p><a href=""></a></p>
    <p><a href=""></a></p>
    <p><a href=""></a></p>
    <p><a href=""></a></p>

</div>

</div> -->