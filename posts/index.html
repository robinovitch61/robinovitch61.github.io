<!DOCTYPE html>
<html lang="en">  
  <head>
    <meta charset="utf-8">
    <title>Posts &ndash; The Leo Zone</title>
    <link rel="icon" href="/img/favicon.png" type="image/x-icon">
    
    <meta name="viewport" content="width=device-width, initial-scale=1">

    
    <link rel="stylesheet" href="https://robinovitch61.github.io/sass/main.min.ff3fd8b1f546481d3e6e4e86779a2bcdde5d3452c4ffcc71680d3781e09fc32f.css">

    

<script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$','$$'], ['\\[', '\\]']],
      processEscapes: true,
      processEnvironments: true
    },
    options: {
      skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
    }
  };

  window.addEventListener('load', (event) => {
      document.querySelectorAll("mjx-container").forEach(function(x){
        x.parentElement.classList += 'has-jax'})
  });
</script>

<script type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

  </head>

  <body>
    <header>
  <div class="container">
    <div class="nav-container">
      <div class="nav-left">
        <a class="btn-text" href="/"><h1>Leo</h1></a>
      </div>

      <div class="dropdown">
        <input class="hamburger menu-btn" type="checkbox" id="menu-btn" />
        <label class="hamburger menu-icon" for="menu-btn">
          <span class="navicon-top"></span>
          <span class="navicon-mid"></span>
          <span class="navicon-bottom"></span>
        </label>
        <div class="menu">
          <a class="menu-item" href="/">Home</a>
          <a class="menu-item" href="/about">About</a>
          <a class="menu-item" href="https://github.com/robinovitch61" target="_blank">
            <img class="social-media-thumbnail" src="/img/github.png" alt="github" title="github" />
          </a>
        </div>
      </div>
    </div>
  </div>
</header>

    
    <div class="top-title">
      <div class="container">
        <h2>Posts</h2>
      </div>
    </div>

    <div class="post-date">
      <div class="container">
        <p></p>
      </div>
    </div>

    <div class="container">
      
<h1>DEFAULT LIST</h1>

  <article>
    <h2>color commentary</h2>
    
<figure>
  <img style="max-width: 120%;" src="/img/cc_collage.png" alt="cc_collage.png" />
  <figcaption class="figcaption-text">Twitter bird - blue or purple?</figcaption>
  
</figure>
<p>About 6 weeks ago, I launched <a href="https://colorcontroversy.com">colorcontroversy.com</a>. Since then, the site has collected over 800,000 color judgements and been shared around the web more than I expected.</p>
<p>A much-simplified version of Randal Monroe of XKCD&rsquo;s <a href="https://blog.xkcd.com/2010/05/03/color-survey-results/">color survey</a>, the site shows you a color shade and presents two color choices for that shade. For example, it shows you a <span style="color: #00878e;">dark turquoise</span> and asks you if it&rsquo;s <span style="color: limegreen;">Green</span> or <span style="color: #547cff;">Blue</span>. Then it shows you what other people thought it was.</p>

<figure>
  <img style="max-width: 70%;" src="/img/turquoise-result.png" alt="turquoise-result.png" />
  
  
</figure>
<p>This the first thing I&rsquo;ve put out there that I wanted lots of random people to use and enjoy. I learned a lot from building it, some of which is discussed below.</p>
<h2 id="dont-design-in-htmlcss">don&rsquo;t design in html/css</h2>
<p>Unless you&rsquo;re <a href="https://lynnandtonic.com/">Lynn Fisher</a>, it can be pretty difficult to translate designs from your head immediately into code. This is obvious to any designer or artistic person, but these things I am not.</p>

<figure>
  <img style="max-width: 70%;" src="/img/figma-cc-mobile.png" alt="figma-cc-mobile.png" />
  <figcaption class="figcaption-text">Note the original skull logo</figcaption>
  
</figure>
<p>Turns out clicking and dragging is a lot easier than CSS when in ideation. The mocks were mostly done before I wrote a line of code. I was able to get feedback from friends on a real-looking site without any real up-front investment.</p>
<h2 id="sign-up-isnt-just-for-data-collection">sign up isn&rsquo;t just for data collection</h2>
<p>The things that take my breath away on the internet almost always load and immerse quickly. A <a href="https://paveldogreat.github.io/WebGL-Fluid-Simulation/">fluid simulation</a> or a <a href="https://www.pixelthoughts.co/">peaceful meditation</a> - no sign up flow, no pop ups or ads, no cookie consent required, nothing between you and the experience.</p>
<p>Notably, the examples above deliver the entire content to the users' browser up front, then no further interaction with the internet is required.</p>
<p>Color Controversy does not behave this way. It collects repeated user input and persists it in a far-away server. So what if someone wants to spam the site with repeated requests from an automated script? What if someone wants to DDoS the site just for the LuLz?</p>
<p>A h4ck3r would have nothing to gain from doing this, but I felt I should operate in the &ldquo;whatever users can do, they will do&rdquo; mindset. I could have embedded a unique token in the browser that the user would automatically include in their data submission when operating the site as intended, but how to obtain the token in a way some script couldn&rsquo;t reproduce fairly easily?</p>
<p>When it comes down to it, deterrence for malicious action is limited without user-specific identification, i.e. sign up and login. Turns out there&rsquo;s more to sign up than just pernicious data collection for profit!</p>
<p>Sign up and login were a deal breaker for me user-experience wise, so what I settled for was implementing server-side <a href="https://github.com/robinovitch61/color-controversy/blob/master/backups/backup.sh">periodic backups of the database</a> with the knowledge that I could at least restore the data to a valid state if someone did decide to mess with it. Luckily, this hasn&rsquo;t been necessary so far.</p>
<h2 id="done-is-better-than-perfect">done is better than perfect</h2>
<p>Could I have worked harder to figure out how to get the play-swagger plugin to parse my Enum color values so I wouldn&rsquo;t have to replicate them manually on the frontend? Yes.</p>
<p>Is the color label text sometimes a little cut off on color labels on the results chart when the color is medium-controversial? Yes.</p>
<p>Are two different docker-compose files, one for dev and one for prod, the best way to do something resembling dev ops? Could I have pushed my built images to remote storage to avoid rebuilding them locally every time? Could I have figured out how to get my Scala Play container to compile and run on less RAM and saved a few dollars a month on AWS costs? Yes, yes, and yes.</p>
<p>But it&rsquo;s done! It works! People use it, and no one has submitted angry github issues denigrating my intelligence. In fact, I got my <a href="https://github.com/robinovitch61/color-controversy">first 3 github stars</a> on the project! I love it as is and am ready to move on.</p>
<h2 id="design-data-collection-early">design data collection early</h2>
<p>I did something really smart and/or really dumb in designing the data model for Color Controversy.</p>
<p>I decided that the database would never grow past its <a href="https://github.com/robinovitch61/color-controversy/blob/master/postgres/init.sql">original number of rows</a> - color submissions simply trigger a SQL query that mutates one of the values in the row of that color:</p>
<div class="highlight"><div style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4">
<table style="border-spacing:0;padding:0;margin:0;border:0;width:auto;overflow:auto;display:block;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sql" data-lang="sql"><span style="color:#008000;font-weight:bold">UPDATE</span> color
<span style="color:#008000;font-weight:bold">SET</span>    n_first <span style="color:#666">=</span> <span style="color:#008000;font-weight:bold">CASE</span> <span style="color:#008000;font-weight:bold">WHEN</span> (color.hex <span style="color:#666">=</span> <span style="color:#ba2121">&#39;${hexColor}&#39;</span> <span style="color:#008000;font-weight:bold">AND</span> color.first_option <span style="color:#666">=</span> <span style="color:#ba2121">&#39;${choice}&#39;</span>) <span style="color:#008000;font-weight:bold">THEN</span> n_first <span style="color:#666">+</span> <span style="color:#666">1</span> <span style="color:#008000;font-weight:bold">ELSE</span> n_first <span style="color:#008000;font-weight:bold">END</span>
     , n_second <span style="color:#666">=</span> <span style="color:#008000;font-weight:bold">CASE</span> <span style="color:#008000;font-weight:bold">WHEN</span> (color.hex <span style="color:#666">=</span> <span style="color:#ba2121">&#39;${hexColor}&#39;</span> <span style="color:#008000;font-weight:bold">AND</span> color.second_option <span style="color:#666">=</span> <span style="color:#ba2121">&#39;${choice}&#39;</span>) <span style="color:#008000;font-weight:bold">THEN</span> n_second <span style="color:#666">+</span> <span style="color:#666">1</span> <span style="color:#008000;font-weight:bold">ELSE</span> n_second <span style="color:#008000;font-weight:bold">END</span>
</code></pre></td></tr></table>
</div>
</div><p>I know Postgres will handle this atomically and liked the idea of a database that was basically the same size no matter how large the traffic grew. No indexing or normalization required when you only have ~100 rows of data üòé.</p>
<p>Unfortunately, someone online pointed out soon after launch that it would have made for some interesting data to collect a little more than just color judgements. Browser usage, mobile/tablet/desktop, general location and time of day along with judgement made makes for some interesting data analysis after the fact.</p>
<p>Fortunately, completely by accident, I can pretty much parse all this out of the Nginx logs, which I started backing up periodically soon after I realized my mistake. They are sitting around ready for me to sift through at some future time.</p>
<p>Unfortunately, now I&rsquo;m sort of treating logs like a read-only database, and I maybe could have just designed this in to my Postgres schema in the first place without taking much of a performance hit. You win some, you lose some.</p>
<h2 id="put-your-stuff-out-there">put your stuff out there</h2>
<p>The internet is cool and building dumb stuff is fun. Making for the web is also kind of terrifying, because it&rsquo;s probably going to be around for a long time. Once you unleash it, it&rsquo;s out there, ready for judgement and criticism. Write my name on it and people&rsquo;s negative reaction to it could be thinly veiled negative reactions to <span class="word-break">MeAndMyStupidDumbBrainWhyWouldIEvenTryToMakeAThingAnywayGahhhhhhh</span>.</p>
<p>I remember eagerly refreshing my first reddit submission for the site and it being downvoted to 0.</p>
<p>Then someone with a million twitter followers tweets it and getting 200k color judgements in a day.</p>
<p>Then I read harsh criticism of it on a french speaking early 2000&rsquo;s-looking forum.</p>
<p>Then a niche Italian art blog emails me asking for permission to write about it for an article.</p>
<p>Then I watch as no one engages with it at all for 10 sequential hours.</p>
<p>Then John Austin, whose <a href="https://www.youtube.com/watch?v=AS1OHMW873s">color theory talk</a> I reference in the <code>What?</code> section of the site, tells me the site reached him before I could tweet at him about it.</p>
<p>Wild ride, I&rsquo;ll tell ya!</p>
<h2 id="in-colorclusion">in colorclusion</h2>
<p>It was fun and worth it and I&rsquo;ll do it all again with something new! I may make a follow-up post at some point in the future once I parse through all the logs and make something pretty with them.</p>

  </article>

  <article>
    <h2>covid projects</h2>
    <p>I have been very fortunate to remain employed during this global pandemic. My productivity working from home has been about on par with what I had in the office, although my learning rate has gone down a bit due to the lack of water-cooler style chats about the product and software in general. We use Slack for remote chat and it feels a bit too formal to trigger notifications for offhand questions.</p>
<p>My free time has been sometimes well-spent, sometimes less so. One of the efforts I&rsquo;m proud of is <a href="https://covid19medstudents.ca/">covid19medstudents.ca</a>. In the spirit <a href="https://robinovitch61.github.io/posts/gifting-websites/">gifting websites</a> and leveraging tech to help the COVID relief effort in some way, I collaborated with a good friend from home to make this site. My friend is president of his medical student&rsquo;s society, and along with a talented designer on his team (<a href="http://www.nancyduanart.com/">Nancy Duan</a>, <a href="https://www.instagram.com/nancyduanart/?hl=en">@nancyduanart</a>), we created an online index of volunteer opportunities and resources aimed at medical students and physicians in Western Canada. It has connected physicians and med students to ~50 projects across 3 provinces over the last couple months.</p>

<figure>
  <img style="max-width: 95%;" src="/img/covid19medstudentshome.png" alt="covid19medstudentshome.png" />
  <figcaption class="figcaption-text">The landing page is very pretty IMO</figcaption>
  
</figure>
<p>It was a satisfying effort. I was feeling quite useless during the early stages of the pandemic, being neither essential nor truly negatively effected. I enjoyed working with a friendly client to fulfill a vision, working with a skilled designer to make things look great, and hearing about volunteers getting connected to projects through the site.</p>
<p>I&rsquo;ve also had the chance to be a Section Leader for Stanford&rsquo;s online &ldquo;Code In Place&rdquo; class. The instructors of CS106A, a relatively well-known intro to programming course offered by Stanford, decided to run a remote &ldquo;first half of the course&rdquo; over 6 weeks of instruction during COVID. Over 5 of those weeks, I met up with ~8 students via zoom as a teaching assistant and we worked through Python challenges together. Participants were brand new to programming and from a huge variety of backgrounds. Seeing them grow and progress over the 5 weeks from learning what a variable was to writing their own text parsers was so very cool.</p>
<p>Another fun project has been taking on the semi-famous <a href="https://cryptopals.com/">cryptopals cryptography challenges</a>. Many sharp folks at the <a href="https://www.recurse.com/">Recurse Center</a> were working on these during my time there and I had heard it was a good way to learn a new programming language. I&rsquo;m <a href="https://github.com/robinovitch61/cryptopals/tree/master/src">solving them in Scala</a> as that&rsquo;s the primary language used at my work, and I&rsquo;ve also taken the opportunity to encorporate a test driven development approach to the challenges. My tests vary in quality, from <code>&quot;frequencyScore&quot; should &quot;be smaller for more englishey things&quot;</code> to <code>&quot;encodeToHexWithXorVigenere&quot; should &quot;encode correctly&quot;</code>.</p>
<p>I&rsquo;ve absolutely loved the format of the challenges - clear and seemingly &ldquo;simple enough&rdquo; questions with very little theoretical explanations, leaving the solver to do the digging and self-education required to make sense of things. For instance, in set 1, question 6, they walk you through a method of finding first the key length, then the key itself for a ciphertext encoded using a repeated-key XOR cipher. Things they don&rsquo;t tell you outright:</p>
<ul>
<li>Given a collection of characters, estimating a probability that the characters were sampled from English or from random garbage text is both a science and an art. They recommend frequency scoring, and my final <code>frequencyScore</code> function included character frequency comparisons to the English language as well as checks for <code>allCharsInAsciiRange</code> and <code>hasReasonableNumSpecialChars</code>. I also experimented with number of spaces in the text sample, to less than excellent results. It felt like getting a good model for this was half the challenge.</li>
<li>The challenge talks about computing and maybe averaging the hamming distance of ciphertext snippets to get the key length. The hamming distance is the number of differing bits. Three equal-length strings, A, B, and K, have the same number of bits. The XOR operation is 1 only if the bits are different, so you can get the hamming distance by counting the number of ones after XOR&rsquo;ing two strings. Note that <code>num_1_bits(A XOR K XOR B XOR K) = num_1_bits(A XOR B)</code>. Also, since English has non-uniform character frequency, the hamming distance of two english snippets is lower than that of two garbage snippets, as english is more similar to english than random text is to random text. All of this was very non-obvious to me and required reading and experimenting a decent amount! Averaging the hamming distance was also definitely necessary to get good results across other encoded text with different keys.</li>
<li>Cracking repeating-key XOR is relatively probabilistic. The longer the ciphertext in relation to the key, the better shot at decoding you&rsquo;ve got! It was easy for me to spend too much time optimizing for instantly cracking all future possible key/text combos - I had to say enough was enough and move on at some point.</li>
</ul>
<p>Here is some code I cracked:</p>
<div class="highlight"><div style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4">
<table style="border-spacing:0;padding:0;margin:0;border:0;width:auto;overflow:auto;display:block;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">0b3637272a2b2e63622c2e69692a23693a2a3c6324202d623d63343c2a26226324272765272a282b2f20430a652e2c652a3124333a653e2b2027630c692b20283165286326302e27282f
</code></pre></td></tr></table>
</div>
</div><p>is hex-encoded ciphertext for repeating key XOR of:</p>
<blockquote>
<p>Burning &lsquo;em, if you ain&rsquo;t quick and nimble\nI go crazy when I hear a
cymbal</p>
</blockquote>
<p>encoded with</p>
<div class="highlight"><div style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4">
<table style="border-spacing:0;padding:0;margin:0;border:0;width:auto;overflow:auto;display:block;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">ICE
</code></pre></td></tr></table>
</div>
</div><p>üòÅ</p>
<p>I&rsquo;ve also been playing a good amount of Zelda: Breath of the Wild, Rocket
League, and Super Smash Bros. Finally, I&rsquo;m now a plant Dad.</p>

<figure>
  <img style="max-width: 95%;" src="/img/plontz.JPG" alt="plontz.JPG" />
  <figcaption class="figcaption-text">Proud of my lil&#39; guys!</figcaption>
  
</figure>

  </article>

  <article>
    <h2>gifting websites</h2>
    <p>Over the past year or so, I have gifted friends a few websites. I think the sites are fun, relatively unique gifts. As a bonus serve, they illustrate some of my progression with basic html/css/javascript static websites.</p>
<h2 id="site-1-averyrobinscomhttpswwwaveryrobinscom">Site 1: <a href="https://www.averyrobins.com">averyrobins.com</a></h2>

<figure>
  <img style="max-width: 100%;" src="/img/averyrobins.png" alt="averyrobins.png" />
  
  
</figure>
<p>I set up an Amazon S3 bucket for static hosting, bought a namecheap domain name, configured the custom DNS, and voila - I now pay $1/month to host this profanity-laden ASCII art. 90% of the time went to setting up, hosting and routing the content - I spent about 20mins on the actual index.html file.</p>
<p>When my content first appeared when I typed <a href="https://averyrobins.com">averyrobins.com</a></a> in to my browser, I was so stoked. I was suddenly a contributor to the previously untouchable monolith of public knowledge, beauty and idiocy that is the internet.</p>
<p>Avery was a fan! Much as I felt when this site went live, it&rsquo;s generally pretty cool to have a public URI that points out your existence. Anyone can see it anywhere from any connected device. For me, this also brings a healthy level of discomfort.</p>
<h2 id="site-2-lettikittelcomhttpslettikittelcom">Site 2: <a href="https://lettikittel.com">lettikittel.com</a></h2>

<figure>
  <img style="max-width: 100%;" src="/img/lettikittel.png" alt="lettikittel.png" />
  
  
</figure>
<p>A similar gift exchange with the same group of friends rolled around this year, this time for Valentine&rsquo;s Day - a &ldquo;sexy&rdquo; gift exchange. I decided to step up my website gifting game with some basic interactivity. What&rsquo;s a sexier gift than a static website with some basic javascript?</p>
<p>Since the days of averyrobins.com, I had taken a few courses on <a href="https://scrimba.com/">scrimba.com</a> and learned how to style up a mostly-responsive site by building this one you&rsquo;re on now. My site is currently hosted through <a href="https://pages.github.com/">Github Pages</a> connected to a namecheap domain. I realized I could save the $1/month AWS fee (and a bunch of setup time) by creating a github account for Letti and doing the same thing there for her site.</p>
<p>Since the setup was so quick, I was actually able to focus on the styling and content of the site a lot more. It was fun to mess around with something that ended up pretty much looking like a customized MySpace page.</p>
<h2 id="site-3-katerinejimenezcomhttpskaterinejimenezcom">Site 3: <a href="https://katerinejimenez.com">katerinejimenez.com</a></h2>

<figure>
  <img style="max-width: 100%;" src="/img/katjimenez.png" alt="katjimenez.png" />
  
  
</figure>
<p>Kat was hosting her blog on wix.com, and without paying the annual fee, the wixsite.com subdomain was displayed in the URI. How could anyone take the content seriously with such a subdomain!? It was absolutely necessary for me to invest hours into porting the content to an entirely new platform.</p>
<p>I made Kat buy the domain name on namecheap and gave her a list of Jekyll themes. She picked one she liked and I emulated the layout and style, again hosting it on Github Pages. I think it looks quite nice.</p>
<p>In conclusion, websites can make a great gift for any occasion. These little projects taught me a lot about how the web works&ndash;I started from quite an unknowledgeable place&ndash;and I enjoyed making them a lot!</p>

  </article>

  <article>
    <h2>givewell is awesome</h2>
    <p><em>Note: this is my personal commentary and this post is not officially endorsed by Givewell in any way.</em></p>
<p><a href="https://www.givewell.org/">Givewell</a> is one of the most admirable philanthropic organizations out there. They recognize and act upon a number of important truths:</p>
<ul>
<li>Money is a super powerful tool for reducing harm in the world</li>
<li>Money can be used in more and less effective ways</li>
<li>The effectiveness of money can be modeled with finite (possibly wide) uncertainty bounds at a non-trivial confidence level</li>
<li>Encouraging those with the means to donate money to effective causes is important and worthwhile</li>
<li>Moral judgement is an inseparable part of judging the effectiveness of an outcome</li>
</ul>
<p>Not only does Givewell maintain an active <a href="https://blog.givewell.org/">blog</a>, a <a href="https://www.givewell.org/giving101">Giving 101</a> section of its site, run free events in the Bay Area every few months and deliver well-curated <a href="https://www.givewell.org/formstack/email-signup">email updates</a>, but they keep updated copies of their <a href="https://www.givewell.org/how-we-work/our-criteria/cost-effectiveness/cost-effectiveness-models">Cost Effectiveness Models</a> and <a href="https://docs.google.com/document/d/1ZKq-MNU-xtn_48uN33L6VvBEZRAduvjwWMeaEffL4K4/edit">Guide to Cost Effective Analysis</a> available online for everyone to view. Transparency is <a href="https://www.givewell.org/how-we-work/transparency">explicitly a core value of the organization</a>.</p>
<p>After digging through some of this available content in detail, I&rsquo;d like to lay out a few key points and assumptions as clear a manner as possible.</p>
<p>First, they talk about the relationship between income ($$$) and consumption (value of used goods and services). It is difficult to infer one from the other - for example, a household&rsquo;s crop production may contribute to consumption (if they consume the crops), income (if they sell the crops), or both.</p>
<p>Givewell states:</p>
<blockquote>
<p>When modeling interventions that we expect will alter people&rsquo;s productivity or work success, we prefer to think in terms of consumption rather than income&hellip;Consumption measurements are in some sense &ldquo;the gold standard&rdquo; for measures of material well-being in development economics research. (<a href="https://docs.google.com/document/d/1ZKq-MNU-xtn_48uN33L6VvBEZRAduvjwWMeaEffL4K4/edit#">source</a>)</p>
</blockquote>
<p>Unfortunately, research often only measures income, as consumption is more difficult to reliably quantify. A reasonable approximation is to assume that consumption and income are equivalent:</p>

<figure>
  <img style="max-width: 80%;" src="/img/income_consumption.png" alt="income_consumption.png" />
  
  
</figure>
<p>From this point forward in my discussion and in all Givewell documentation, it is reasonable to think of income and consumption as equivalent unless otherwise stated. I will arbitrarily choose to use the term consumption to represent both.</p>
<p>When examining consumption in relation to well-being (well-being representing self-reported happiness, decrease in suffering, hope for the future, etc.), it&rsquo;s intuitively true that increasing consumption linearly would yield diminishing returns in well-being. At low levels of consumption, an extra \$10/day may result in a doubling or tripling of one&rsquo;s well being! At high levels of consumption, an extra \$10/day may feel almost like an insult if its meant to measurably increase ones well-being - the economic equivalent of <a href="https://www.youtube.com/watch?v=cW-wTkUBBeo">&ldquo;you want a cookie or something?&quot;</a>.</p>
<p>For each base on the plot below, it can be said that &ldquo;well-being increases an equivalent amount (one unit) each time consumption is multiplied by {the base}&rdquo;.</p>

<figure>
  <img style="max-width: 80%;" src="/img/consumption_dim.png" alt="consumption_dim.png" />
  
  
</figure>
<p>Another idea: rather than a linear increase in consumption increasing well-being a proportional amount, a doubling in consumption increases well-being a proportional amount. So \$10/day to \$20/day increases well-being an equivalent amount as \$5,000/day to \$10,000/day. This is a logarithmic relationship, in this case base 2 for a doubling. In practice, Givewell uses base $e \approx 2.72$, which isn&rsquo;t much different.</p>

<figure>
  <img style="max-width: 80%;" src="/img/log_consumption_lin.png" alt="log_consumption_lin.png" />
  
  
</figure>
<p>This is assumption is supported in <a href="http://users.nber.org/~jwolfers/papers/Satiation(AER).pdf">this paper</a> which provides the plot shown below. Givewell recognizes the uncertainty here:</p>
<blockquote>
<p>&ldquo;The actual relationship between well-being and wealth is hotly debated.&rdquo; (<a href="https://docs.google.com/document/d/1ZKq-MNU-xtn_48uN33L6VvBEZRAduvjwWMeaEffL4K4/edit#">source</a>)</p>
</blockquote>

<figure>
  <img style="max-width: 80%;" src="/img/wellbeing_gdp.png" alt="wellbeing_gdp.png" />
  
  
</figure>
<p>The above research shows one measure on the y-axis: the satisfaction ladder, a measure of self-reported happiness. Givewell extrapolates this research such that the y-axis can just represent well-being of any metric. This is reasonable as long as the assumption that a linear-logarithmic relationship exists between consumption and the metric serving as a proxy for well-being.</p>
<p>The constant slope of the linear-logarithmic between well-being and consumption is something useful. It represents the <em>increase in well-being per unit increase in $ln(consumption)$</em>.</p>
<p>Givewell asks &ldquo;what would be the increase in well-being per a doubling of consumption from $c_1$ to $2c_1$?&rdquo;. This doubling of consumption is chosen arbitrarily and doesn&rsquo;t have anything to do with the base of the model - logarithmic base remains $e$.</p>
<p>$$ Slope = \frac{x}{\ln{2c_1} - \ln{c_1}} $$
$$ = \frac{x}{\ln{\frac{2c_1}{c_1}}} $$
$$ = \frac{x}{\ln{2}} $$
$$ \approx 1.443x $$</p>
<p>What is $x$? Well, $x$ is the amount well-being increased when consumption doubled. This could be read off the satisfaction ladder plot above, but then the slope would be valid only for that particular well-being metric. Instead, Givewell defines $x$ as the <span class="italic">general value assigned to doubling consumption for one person for one year</span>.</p>
<p>With this definition, say self-reported happiness increases by 3 units every time consumption doubles. The value of increasing happiness by 6 units caused by a quadrupling of consumption would then be $2x$. Consumption doubled twice, leading to twice the general value of doubling it once.</p>
<p>Slope, a function of $x$, can now be used to calculate the increase in well-being $\Delta WB$. Say an intervention causes a consumption increase of 4 times for one person in one year:</p>
<p>$$ \Delta WB = slope*(\ln(4c_1) - \ln(c_1)) $$
$$ \Delta WB = \frac{1}{\ln{2}}x*(\ln\frac{4c_1}{c_1}) $$
$$ \Delta WB = \frac{\ln4}{\ln{2}}x $$
$$ \Delta WB = 2x $$</p>
<p>Recall that x is the general value assigned to doubling consumption for one year. Increasing consumption by 4 times for one person for one year results in a well-being increase of twice the value of only doubling consumption.</p>
<p>Keeping things general like this may seem arbitrary and overly confusing now, but the real value of this method comes in when moral judgement is brought in to play. Say one would like to compare the value of doubling consumption for an individual in one year to the value of saving a young childs life. If one judges that saving the child&rsquo;s life is 50 times as good as doubling consumption, then set the value of that outcome to 50$x$! After some math, outcomes can be compared to one another.</p>
<p>I&rsquo;d like to continue digging through Givewell&rsquo;s open sourced materials. Likely future additional writing/projects to come.</p>

  </article>

  <article>
    <h2>concurrency in python</h2>
    
<figure>
  <img style="max-width: 70%;" src="/img/concurrency.png" alt="concurrency.png" />
  <figcaption class="figcaption-text">Concurrency ! easy is</figcaption>
  
</figure>
<p>Concurrency is of paramount importance in modern computing. It is a complex domain in itself, with major contributions from database development, machine learning, distributed application deployment, and academia.</p>
<p>Python requires a clear and simple concurrency framework as it is commonly used in high-compute settings like training neural networks. Matrix algebra has been made fast with NumPy and related tools, interfacing with C to get large performance improvements. In the machine learning context, Python often combines concurrency with this efficient toolset to yield readable, easily iterated-upon, and rapid results.</p>
<p>Modern computers already use concurrency for almost everything. It is likely that the machine you&rsquo;re reading this on has multiple cores/CPUs, each of which runs multiple processes (multiprocessing). Processes are just programs given a &ldquo;time chunk&rdquo; in which to run on a CPU, after which their state is cached efficiently while the next program runs. If a process is running and itself needs to do multiple things (e.g. show a loading icon while also actually loading the content), a process gets split in to threads, which themselves get allocated time chunks from the overall process&rsquo;s time chunk. Processes usually run separate from and possibly in parallel with other processes, with strictly defined ways of interacting with one another.</p>
<p>Underlying the original CPython interpreter is the Global Interpreter Lock (GIL). This ensures that the Python interpreter is controlled by only one thread at a time. This is mainly because Python cleans up memory by &ldquo;reference counting&rdquo; instead of other means like garbage collection, ownership, etc. If the reference count for an object drops to zero, Python releases the memory allocated for that object. But multiple threads could change the reference count for an object at the same time, possibly causing memory errors and weird bugs. The GIL is a single lock on the Python interpreter, ensuring that memory is properly managed and no <a href="https://en.wikipedia.org/wiki/Deadlock">deadlocks</a> occur. Because Python was designed decades ago and parallel processing wasn&rsquo;t as high priority, this solution to memory management assumption wasn&rsquo;t seen as a big deal back then.</p>
<p>Unfortunately, this means that multithreading for CPU-intensive tasks in Python doesn&rsquo;t actually expediate computation - you can write code that implies threads are helping to speed up a high CPU task, but the GIL will ensure that only one thread runs at a time anyway! In fact, multithreading on high CPU tasks will actually be SLOWER than a single thread, as the GIL has decently high overhead when locking/unlocking threads. The one place multithreading is advantageous is IO operations, as IO operations <a href="https://stackoverflow.com/questions/29270818/why-is-a-python-i-o-bound-task-not-blocked-by-the-gil">do not require the lock from the GIL</a>.</p>

<figure>
  <img style="max-width: 70%;" src="/img/cpu_io_bound.png" alt="cpu_io_bound.png" />
  
  <figcaption class="figcaption-text">Source: <a href="https://realpython.com/python-concurrency/">https://realpython.com/python-concurrency/</a></figcaption>
</figure>
<p>The degredation of performance with threaded compute and improvement in performance with threaded IO is demonstrated in the following static notebook:</p>
<script src="https://gist.github.com/robinovitch61/e9f94dabc8d46b269a279759676596a6.js"></script>
<p>Race conditions occur when data is modified or dropped in an undesirable manner due to a process switching threads before read-modify-write code sections are complete! Here is a complete example of a race condition and how to fix it using locks:</p>
<script src="https://gist.github.com/robinovitch61/8bf045f09ab77e26527139b914fa55d9.js"></script>
<p>Threading is all well and good, but what if we need true parallel operation on CPU bound computation to speed up a general task? That&rsquo;s where the <code>multiprocessing</code> built in package comes in. Processes have more overhead than threads, but get around the GIL-caused limitations of multithreading in CPython, allowing real compute performance improvements. Additionally, processes don&rsquo;t share global variables the same way threads do. Data sharing between processes is possible but explicitly specified in the code, making them generally safer. Threads are subject to race conditions and deadlocks, but these are more easily avoided with processes.</p>
<p>An example of how multiprocessing can speed up a CPU bound operation is as follows:</p>
<script src="https://gist.github.com/robinovitch61/380dc5af7c1bf684d5dbd64cac966f7c.js"></script>
<p>I don&rsquo;t cover <code>asyncio</code> here, but that&rsquo;s a topic for another day! I hope <code>threading</code> and <code>multiprocessing</code> were made more clear from the above discussion and examples.</p>
<h2 id="references">References:</h2>
<ul>
<li><a href="https://lerner.co.il">Better Developers Mailing List of https://lerner.co.il/</a></li>
<li><a href="https://realpython.com/python-gil/">https://realpython.com/python-gil/</a></li>
<li><a href="https://medium.com/practo-engineering/threading-vs-multiprocessing-in-python-7b57f224eadb">https://medium.com/practo-engineering/threading-vs-multiprocessing-in-python-7b57f224eadb</a></li>
<li><a href="https://wiki.python.org/moin/GlobalInterpreterLock">https://wiki.python.org/moin/GlobalInterpreterLock</a></li>
<li><a href="https://medium.com/towards-artificial-intelligence/the-why-when-and-how-of-using-python-multi-threading-and-multi-processing-afd1b8a8ecca">https://medium.com/towards-artificial-intelligence/the-why-when-and-how-of-using-python-multi-threading-and-multi-processing-afd1b8a8ecca</a></li>
<li><a href="https://www.geeksforgeeks.org/multithreading-in-python-set-2-synchronization/">https://www.geeksforgeeks.org/multithreading-in-python-set-2-synchronization/</a></li>
<li><a href="https://realpython.com/python-concurrency/">https://realpython.com/python-concurrency/</a></li>
<li><a href="https://realpython.com/intro-to-python-threading/#race-conditions">https://realpython.com/intro-to-python-threading/#race-conditions</a></li>
</ul>

  </article>

  <article>
    <h2>statistical bias</h2>
    
<figure>
  <img style="max-width: 70%;" src="/img/jigsaw.png" alt="jigsaw.png" />
  <figcaption class="figcaption-text">Eat my spheres!</figcaption>
  
</figure>
<p>You find yourself trapped in a room with  Jigsaw, the villain from the Saw movies. There is a wood box of 100 spheres, some of them steel ball bearings and some of them gumballs. Jigsaw tells you that you can leave the room alive if you guess the number of steel ball bearings in the jar correctly. You can pull 10 random spheres out the 100 in the box as many times as you want, but each time you do, you have to eat all of them (yum!). Jigsaw will then replace the stuff you pulled out of the box with new ones - if you pulled 7 ball bearings and 3 gumballs, 7 and 3 are replaced respectively. You&rsquo;re like &ldquo;cool, I got this, I&rsquo;m just going to do this a bunch of times and average the number of ball bearings I have to eat each time, then multiply the average by 10 and go home to my family of 12&rdquo;. Little did you know, Jigsaw has implanted magnets in your fingertips, attracting the ball bearings. Your estimate will be biased no matter how many spheres you eat. You will be killed, and your family will mourn.</p>
<p>Bias, both in common vernacular and in statistics, can be thought of as the difference between the average estimate of some truth and the truth itself. Your cousin is biased because his opinions on the evils of gun control, while varied, are on average far from the truth. A statistical model is biased because the average difference between its fitted and true values are non-zero.</p>
<p>Say you estimate a model parameter, e.g. $ \hat{\beta} $, based off a sample of data. Assuming the data comes from some underlying distribution with true parameter $ \beta $, your estimate $ \hat{\beta} $ is unbiased if the average estimate of it is equal to the true parameter, i.e. $ E[\hat{\beta}] = \beta $.</p>
<p>The concept of bias-variance tradeoff is fundamental in modeling. Intuitively, bias is the average difference in model prediction to true value, as discussed above. Variance is how much the model changes when you alter the sample data.</p>

<figure>
  <img style="max-width: 95%;" src="/img/high_and_low_bias.png" alt="high_and_low_bias.png" />
  <figcaption class="figcaption-text">High and low model bias</figcaption>
  
</figure>
<p>Say, theoretically, that your goal is to have a good model. You might say &ldquo;I want my model&rsquo;s predictions to be close to reality&rdquo;. Even, &ldquo;I want the average squared-error of my models predictions to be minimized!&rdquo;. That&rsquo;s not a bad idea! But wait - the image on the left above looks great for that, right? Why is that not the best model? Because the points that this model is going through don&rsquo;t represent all the possible points - it&rsquo;s just a data sample. If you got some more data, your model might not perform very well - its mean-squared error (MSE) might go up.</p>
<p>Demonstrated visually, 3 models fit the data below. They increase in number of predictors, i.e. flexibility of the model. Note how there is always some model error (dashed line on right) - this is the irreducible error, due to things like randomness in the generation of future data and inability to parameterize the universe. The red line represents our test error, our test set being some data we withheld from the model while estimating its parameters. The grey line represents the training error, based on the data we estimated our model parameters with. Training error CAN go to zero, as in the left image above, but we really care about our test error. Test error represents our model&rsquo;s ability to make accurate future predictions.</p>

<figure>
  <img style="max-width: 95%;" src="/img/bias_variance.png" alt="bias_variance.png" />
  <figcaption class="figcaption-text">The bias-variance tradeoff</figcaption>
  
</figure>
<p>Note that MSE can be mathematically decomposed in to bias and variance terms. Assume for some data sample/training set, it came from same &ldquo;data generating function&rdquo; $y = f(x) + \epsilon $ where $ \epsilon \sim N(0, \sigma^2) $. We want to find the $ \hat{f}(x) $ that results in predictions for $ y $ that minimize MSE for data even outside our sample.</p>
<p>$$ E[(y - \hat{f}(x))^2] = MSE $$
$$ = Bias[\hat{f}(x)]^2 + Var(\hat{f}(x)) + \sigma^2 $$</p>
<p>Proofs for this exist <a href="https://towardsdatascience.com/mse-and-bias-variance-decomposition-77449dd2ff55">here</a> and <a href="https://www.inf.ed.ac.uk/teaching/courses/mlsc/Notes/Lecture4/BiasVariance.pdf">here</a>.</p>
<p>The bias-variance tradeoff has an important consequence: it is possible that an increase in bias can actually decrease variance to the point where overall MSE is better! A good model minimizes MSE, not just bias or variance.</p>
<p>A concrete example of biased estimators exists below. Generating many samples from an underlying function, both MLE and OLS methods result in unbiased estimates of weights:</p>

<figure>
  <img style="max-width: 95%;" src="/img/unbiased_weights.png" alt="unbiased_weights.png" />
  <figcaption class="figcaption-text">The mean estimate for each weight is the same as the red true value</figcaption>
  
</figure>
<p>Importantly, the MLE estimate for error variance $ \sigma^2 $ is biased, whereas the OLS estimate is unbiased. Note that the only difference between the MLE and OLS estimates is dividing by $ m - n $, the number of samples minus the number of predictors. Some intuition for how sample variance is biased without this division can be built from <a href="https://proofwiki.org/wiki/Bias_of_Sample_Variance">this proof</a>, while a more involved proof of the unbiased OLS error variance formula <a href="https://stats.stackexchange.com/questions/20227/why-is-rss-distributed-chi-square-times-n-p">lies here</a>.</p>

<figure>
  <img style="max-width: 95%;" src="/img/biased_mle_error_var.png" alt="biased_mle_error_var.png" />
  <figcaption class="figcaption-text">MLE (left) mean estimate differs from the true value</figcaption>
  
</figure>
<p>Complete code for the simulation can be found in <a href="/ols_vs_mle_bias.html">the static notebook here</a>.</p>
<p>I hope this helped develop a deeper understanding of statistical bias. If you have any additions or errors to correct, please open an issue on the source of this website <a href="https://github.com/robinovitch61/the-leo-zone/issues">here</a>.</p>
<h2 id="references">References:</h2>
<ul>
<li><a href="https://towardsdatascience.com/understanding-the-bias-variance-tradeoff-165e6942b229">https://towardsdatascience.com/understanding-the-bias-variance-tradeoff-165e6942b229</a></li>
<li><a href="http://faculty.marshall.usc.edu/gareth-james/ISL/">http://faculty.marshall.usc.edu/gareth-james/ISL/</a></li>
<li><a href="https://www.youtube.com/watch?v=C3nIFH649wY">https://www.youtube.com/watch?v=C3nIFH649wY</a></li>
<li><a href="https://stats.stackexchange.com/questions/207760/when-is-a-biased-estimator-preferable-to-unbiased-one">https://stats.stackexchange.com/questions/207760/when-is-a-biased-estimator-preferable-to-unbiased-one</a></li>
<li><a href="https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeof">https://en.wikipedia.org/wiki/Bias‚Äìvariance_tradeof</a></li>
<li><a href="https://proofwiki.org/wiki/Bias_of_Sample_Variance">https://proofwiki.org/wiki/Bias_of_Sample_Variance</a></li>
<li><a href="https://www.inf.ed.ac.uk/teaching/courses/mlsc/Notes/Lecture4/BiasVariance.pdf">https://www.inf.ed.ac.uk/teaching/courses/mlsc/Notes/Lecture4/BiasVariance.pdf</a></li>
<li><a href="https://towardsdatascience.com/mse-and-bias-variance-decomposition-77449dd2ff55">https://towardsdatascience.com/mse-and-bias-variance-decomposition-77449dd2ff55</a></li>
</ul>

  </article>

  <article>
    <h2>linear regression</h2>
    <p><em>Update 2019-10-25: Added MLE derivation and OLS $ \sigma^2 $</em></p>
<p>I have learned linear regression a number of times with varying of levels of detail. I&rsquo;m making this document mostly as a reference for myself and any others who may be interested in the technique.</p>
<p>The concepts and explanations here come either from <a href="http://faculty.marshall.usc.edu/gareth-james/ISL/">An Introduction to Statistical Learning (ISLR)</a> or <a href="https://web.stanford.edu/~hastie/ElemStatLearn/">The Elements of Statistical Learning (ESLR)</a>. Other sources are listed at the bottom.</p>

<figure>
  <img style="max-width: 95%;" src="/img/linreg.png" alt="linreg.png" />
  <figcaption class="figcaption-text">Good ol&#39; linear regression</figcaption>
  <figcaption class="figcaption-text">Source: <a href="https://en.wikipedia.org/wiki/Linear_regression">https://en.wikipedia.org/wiki/Linear_regression</a></figcaption>
</figure>
<p>Note that I will put most everything in multiple linear regression matrix format as it extends to any number of predictors, including the $ y = mx+b $ (i.e. $ y = \beta_1x_1 + \beta_0x_0 $) classic simple linear regression.</p>
<h2 id="notation-and-terminology">Notation and Terminology</h2>
<ul>
<li>$\mathbf{m}$: number of observations in data set <br>($\mathbf{m}$ denoted by $\mathbf{N}$ in ESLR and $\mathbf{n}$ in ISLR)</li>
<li>$\mathbf{n}$: number of predictors in data set <br>($\mathbf{n}$ denoted by $\mathbf{p}$ in both ISLR and ESLR)</li>
<li><em>Predictors</em> (also known as features, independent variables, inputs, or explanatory variables): $ x_0 $, $ x_1 $, $ x_2 $&hellip;$ x_{n-1} $ (where $ x_0 = 1 $)</li>
<li><em>Weights</em> (also known as parameters, multipliers, or regression coefficients): $ \beta_0 $, $ \beta_1 $, $ \beta_2 $&hellip;$ \beta_{n-1} $</li>
<li><em>Response</em> (also known as dependent variable): $ y_0, y_1, y_2, &hellip; y_{m-1} $, the actual data values associated with each set of $\mathbf{n}$ predictors</li>
<li><em>Fitted Values</em> (also known as outputs): $\hat{y}_0$, $\hat{y}_1$, $\hat{y}_2$, $\hat{y}_{m-1}$, the predictions of the response given some estimated weights $ \hat{\beta}_0, \hat{\beta}_1, \hat{\beta}_2, &hellip; \hat{\beta}_{n-1} $</li>
</ul>
<h2 id="what-does-linear-mean">What does <em>linear</em> mean?</h2>
<p>All that <em>linear</em> means is that the model is of the following form:</p>
<p>$$f(\mathbf{X}) = \mathbf{X}\beta$$</p>
<p>Here, matrix $\bf{X} \in \mathbb{R}^{m \times n}$, weight vector $\beta \in \mathbb{R}^{n \times 1}$ and the leftmost column of $ \mathbf{X} $ denoted $ X_0 $ is a column of 1s to represent the intercept.</p>
<ul>
<li>Linear regression is often thought of as a &ldquo;straight line fit&rdquo; to a set of observations. It doesn&rsquo;t [necessarily result in a straight line or flat plane, though. Consider that &lt;span class=&ldquo;inline-math](necessarily result in a straight line or flat plane, though. Consider that &lt;span class=&ldquo;inline-math)
The &ldquo;true&rdquo; relationship or &ldquo;population regression line&rdquo; between $ Y \in \mathbb{R}^{m \times 1} $ (the response values associated with each row/observation of $ \mathbf{X} $) and $ \mathbf{X} $ is</li>
</ul>
<p>$$ Y = f(\mathbf{X}) + \epsilon = \mathbf{X}\beta + \epsilon $$</p>
<p>Epsilon ($ \epsilon $) here represents the error term, encompassing omitted and unmeasurable predictors, measurement error of included predictors, and the generic inability of our selected linear model to fit the true relationship.</p>
<h2 id="indicators-that-linear-regression-model-is-lacking">Indicators that Linear Regression Model is Lacking</h2>
<p>After coming up with a linear regression model, there are various required checks in order to validate that the model is acceptable as a description of the data. If any of the following are true, it is necessary to either improve the data, model or accept linear regression is insufficient as a modeling technique under the given conditions.</p>
<p>
<figure>
  <img style="max-width: 95%;" src="/img/anscombe.png" alt="anscombe.png" />
  
  
</figure>

<figure>
  <img style="max-width: 95%;" src="/img/anscombetable.png" alt="anscombetable.png" />
  <figcaption class="figcaption-text">The need for checking assumptions</figcaption>
  <figcaption class="figcaption-text">Source: <a href="https://en.wikipedia.org/wiki/Anscombe%27s_quartet">https://en.wikipedia.org/wiki/Anscombe%27s_quartet</a></figcaption>
</figure></p>
<h3 id="non-linearity-of-the-response-predictor-relationships">Non-linearity of the response-predictor relationships</h3>

<figure>
  <img style="max-width: 95%;" src="/img/residualvsfit.png" alt="residualvsfit.png" />
  <figcaption class="figcaption-text">Left plot shows trend in residuals while right plot indicates better fit</figcaption>
  
</figure>
<p>There should be a linear relationship between the predictors and the response. This can be confirmed or denied by plotting the residuals $ e_i = y_i - \hat{y}_i $ versus the fitted values $ \hat{y}_i $. If the residuals look evenly dispersed about the horizontal access, the model is reasonable. Note that a poor residual plot may just be an indicator of poor predictor selection, e.g. non-linear predictors ($ x^2, \sqrt{x} $, etc.) should be fed in to the model.</p>
<h3 id="correlation-of-error-terms">Correlation of error terms</h3>

<figure>
  <img style="max-width: 95%;" src="/img/correlatederrors.png" alt="correlatederrors.png" />
  <figcaption class="figcaption-text">Correlated errors: adjacent errors tend to be of similar sign and magnitude</figcaption>
  
</figure>
<p>Given ordered data (e.g. time series), plot the residuals in order of time. If patterns arise, it is likely your data has correlated error terms. In time series data, this is also called <a href="https://en.wikipedia.org/wiki/Autocorrelation">autocorrelation</a>. This could occur, for example, if you duplicated your time series dataset - looking at error terms in order, each one would have perfect correlation with the next or previous value! You would get the same parameter fits but drastically different confidence bounds. Other real world examples include sampling biological metrics from the same family or the stock market doing well during certain time periods and poorly in others. At times adding predictors to the model can solve this problem. There are also techniques for removal of autocorrelation with transformation.</p>
<h3 id="non-constant-variance-of-error-terms">Non-constant variance of error terms</h3>
<p>
<figure>
  <img style="max-width: 95%;" src="/img/heteroscedasticity.png" alt="heteroscedasticity.png" />
  
  
</figure>

<figure>
  <img style="max-width: 95%;" src="/img/heteroscedasticity2.png" alt="heteroscedasticity2.png" />
  <figcaption class="figcaption-text">Heteroscedasticity can sometimes be removed by transforming the response</figcaption>
  
</figure></p>
<p>For linear regression&rsquo;s standard errors, confidence bounds, and hypothesis tests, it is assumed that data was generated from a population regression line where the true error $ \epsilon $ is normally distributed with mean 0 and constant variance $ \sigma^2 $, that is $ \epsilon \sim N(0, \sigma^2) $. In reality, it is often the case in that $ \sigma^2 $ varies with the magnitude of the predictors and/or response - this is called heteroscedasticity. Transformation of the response may help the situation. If you have an idea of the variance of error at each response, you can also use weighted least squares.</p>
<h3 id="data-contain-outliers">Data contain outliers</h3>

<figure>
  <img style="max-width: 95%;" src="/img/outlier.png" alt="outlier.png" />
  <figcaption class="figcaption-text">Spot the outlier</figcaption>
  
</figure>
<p>Outlier points have abnormally high residuals. Methods exist for determining what &ldquo;abnormal&rdquo; means here, including examining the studentized residuals (dividing residuals by their estimated standard error). Outliers may not dramatically affect the weights of a model, but can affect the confidence bounds, hypothesis tests and $ R^2 $ value. Removing outliers is sometimes favorable, especially if they stem from something like measurement error. Before removal, it&rsquo;s important to be sure that outliers aren&rsquo;t actually important pieces of data implying that your model is missing predictors or is somehow otherwise deficient.</p>
<h3 id="data-contain-high-leverage-points">Data contain high-leverage points</h3>

<figure>
  <img style="max-width: 95%;" src="/img/leverage.png" alt="leverage.png" />
  <figcaption class="figcaption-text">Leverage: not just for negotiation anymore</figcaption>
  
</figure>
<p>High leverage points have unusual predictor values, e.g. point 41 on the left plot above. Note that in multiple regression it can appear that high leverage points appear normal if you only examine each predictor value individually (middle graph above). Instead, calculate leverage using the diagonal of the &ldquo;projection matrix&rdquo; $ \mathbf{H} = \mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T $. Methods for determining what leverage magnitude is unusual enough to be suspect exist, including the <a href="http://www.lithoguru.com/scientist/statistics/Lecture21.pdf">Williams Graph</a> similar the rightmost plot above.</p>
<h3 id="collinearity">Collinearity</h3>

<figure>
  <img style="max-width: 95%;" src="/img/collinearity.png" alt="collinearity.png" />
  <figcaption class="figcaption-text">Age and limit are not collinear while rating and limit are highly collinear</figcaption>
  
</figure>
<p>Collinearity occurs when 2 or more predictors are highly related. This reduces the accuracy of the estimates of the weights, as it becomes hard to parse out which predictor is having an effect on the response. Detecting collinearity can be done by calculating the <a href="https://en.wikipedia.org/wiki/Variance_inflation_factor">Variance Inflation Factor</a>, and solved by either removing one of the collinear predictors or combining collinear predictors together.</p>
<h3 id="endogeneity">Endogeneity</h3>
<p>Endogeneity occurs when there is correlation between model predictor(s) and the true error term $ \epsilon $. Note that the error term here is NOT the estimated error term $ \hat{\epsilon} $ as a result of the fitted values $ \hat{Y} $, but the true error term in the population regression line $ Y = \mathbf{X}\beta + \epsilon $. An example I found explained it well was the burger regression <a href="https://stats.stackexchange.com/questions/263324/how-can-the-regression-error-term-ever-be-correlated-with-the-explanatory-variab">here</a>. Upon suspicion that a predictor is endogenous, a way to test for it is outlined <a href="https://www.youtube.com/watch?v=61I-8tDgwag">here</a>. Enogeneity is different than heteroscedasticity, as explained <a href="https://stats.stackexchange.com/questions/261329/ols-difference-between-exogeneity-and-homoscedasticity">here</a>.</p>
<h3 id="notes-on--r2--coefficient-of-determination">Notes on $ R^2 $ (coefficient of determination)</h3>

<figure>
  <img style="max-width: 95%;" src="/img/rsquared.png" alt="rsquared.png" />
  <figcaption class="figcaption-text">Increasing $ R^2 $ from left to right</figcaption>
  
</figure>
<p>$ R^2 $ does nothing except measure the proportion of variability in Y that can be explained using X. It can always be made closer to 1 by adding more features/variables into the linear model, which at a certain point likely results in a too-flexible (overfit) model that fails to minimize test error. Where $ R^2 $ can be useful is in confirming previously held beliefs about what you are modeling. For example, modeling something in physics that theoretically should be extremely linear, an $ R^2 $ much smaller than 1 indicates something may be off with the model or data. Conversely, a low $ R^2 $ would actually be expected when modeling a complex situation with high residual errors due to other factors.</p>
<h2 id="normal-equation-for-estimated-weights--hatbeta-">Normal Equation for Estimated Weights $ \hat{\beta} $</h2>
<p>In the Ordinary Least Squares approach, RSS (Residual Sum of Squares) as a function of $ \hat{\beta} $ is the function to minimize:</p>

<figure>
  <img style="max-width: 95%;" src="/img/residuals.png" alt="residuals.png" />
  <figcaption class="figcaption-text">Red lines are residuals $ e_i $. RSS is the sum of the square of all residuals</figcaption>
  
</figure>
<p>$$ RSS = \sum_{i=1}^{m}e_i^2 = \sum_{i=1}^{m}(y_i - \hat{y}_i)^2  = \sum_{i=1}^{m}(y_i - x_i^T\hat{\beta})^2$$
where $y_i$ is the response and $\hat{y}_i = x_i^T\hat{\beta}$ is the fitted value. This is assuming $x_i \in \mathbb{R}^{n \times 1}$ and $\hat{\beta} \in \mathbb{R}^{n \times 1}$ are column vectors and that the first value in each $ x_i $ is 1.</p>
<p>We can rewrite this in matrix form with $\bf{X} \in \mathbb{R}^{m \times n}$ and $y \in \mathbb{R}^{m \times 1}$ as
$$RSS(\hat{\beta})=(y - \mathbf{X}\hat{\beta})^T(y - \mathbf{X}\hat{\beta})$$</p>
<p>Each estimated weight vector $\hat{\beta}$ will give a different $RSS$ value. Since the premise of Ordinary Least Squares (OLS) is minimizing $RSS$, we can take the derivative of $RSS(\hat{\beta})$ and set it to zero to obtain the normal equations for the $\hat{\beta}$ that minimizes $RSS$. Note that we use $ (\mathbf{AB})^T = \mathbf{B}^T\mathbf{A}^T $ and the fact that matrices are distributive:
$$RSS(\hat{\beta})=(y^T - (\mathbf{X}\hat{\beta})^T)(y - \mathbf{X}\hat{\beta})$$
$$RSS(\hat{\beta})=(y^T - (\hat{\beta}^T\mathbf{X}^T))(y - \mathbf{X}\hat{\beta})$$
$$RSS(\hat{\beta})=y^Ty - y^T\mathbf{X}\hat{\beta} - (\hat{\beta}^T\mathbf{X}^T)y + \hat{\beta}^T\mathbf{X}^T\mathbf{X}\hat{\beta}$$</p>
<p>Taking the derivative with respect to the estimated weights:
$$ \frac{\partial}{\partial{\hat{\beta}}}RSS(\hat{\beta}) = $$
$$
\color{grey}{\frac{\partial}{\partial{\hat{\beta}}}y^Ty}
\color{black}{-}
\color{red}{\frac{\partial}{\partial{\hat{\beta}}}(y^T\mathbf{X}\hat{\beta} + (\hat{\beta}^T\mathbf{X}^T)y)}
\color{black}{+}
\color{green}{\frac{\partial}{\partial{\hat{\beta}}}\hat{\beta}^T\mathbf{X}^T\mathbf{X}\hat{\beta}}
$$</p>
<p>Now luckily, $\color{grey}{\frac{\partial}{\partial{\hat{\beta}}}y^Ty}=0$ as response $y$ is independent of $\hat{\beta}$. In addition, the red term isn&rsquo;t so bad. Because $RSS$ is scalar, each term is a scalar. And since scalars are symmetric, $a^T=a$. Again, we use $ (\mathbf{AB})^T = \mathbf{B}^T\mathbf{A}^T $.</p>
<p>So the following holds:
$$\color{red}{   \frac{\partial}{\partial{\hat{\beta}}} (y^T\mathbf{X}\hat{\beta} + (\hat{\beta}^T\mathbf{X}^T)y) }$$
$$\color{red}{ = \frac{\partial}{\partial{\hat{\beta}}} ((y^T\mathbf{X}\hat{\beta})^T + (\hat{\beta}^T\mathbf{X}^T)y) }$$
$$\color{red}{ = \frac{\partial}{\partial{\hat{\beta}}} ((\mathbf{X}\hat{\beta})^Ty + (\hat{\beta}^T\mathbf{X}^T)y) }$$
$$\color{red}{ = \frac{\partial}{\partial{\hat{\beta}}} ((\hat{\beta}^T\mathbf{X}^T)y + (\hat{\beta}^T\mathbf{X}^T)y) }$$
$$\color{red}{ = \frac{\partial}{\partial{\hat{\beta}}} 2\hat{\beta}^T\mathbf{X}^Ty }$$</p>
<p>We can take advantage of the &ldquo;A is not a function of x&rdquo; identity <a href="https://en.wikipedia.org/wiki/Matrix_calculus#Vector-by-vector_identities">here</a>, $ \frac{\partial{x^T\mathbf{A}}}{\partial{x}} = \mathbf{A} $. There is a great intuitive walkthrough of this identity <a href="https://eli.thegreenplace.net/2015/the-normal-equation-and-matrix-calculus/#id3">here</a> if interested.
$$\color{red} {2\frac{\partial\hat{\beta}^T\mathbf{X}^Ty}{\partial{\hat{\beta}}} = 2\mathbf{X}^Ty }$$</p>
<p>The green term uses another identity, &ldquo;A is not a function of x, A is symmetric&rdquo; <a href="https://en.wikipedia.org/wiki/Matrix_calculus#Scalar-by-vector_identities">here</a>, $ \frac{\partial{x^T\mathbf{A}x}}{\partial{x}} = 2\mathbf{A}x $. Note that this relies on the $ \mathbf{A} $ matrix being symmetric, but since $ \mathbf{X}^T\mathbf{X} $ is indeed symmetric, this holds. Again, there is a great intuitive walkthrough of this other identity <a href="https://eli.thegreenplace.net/2015/the-normal-equation-and-matrix-calculus/#id3">here</a> if interested as well.
$$ \color{green} {\frac{\partial\hat{\beta}^T\mathbf{X}^T\mathbf{X}\hat{\beta}}{\partial{\hat{\beta}}} = 2\mathbf{X}^T\mathbf{X}\hat{\beta} }$$</p>
<p>Finally, we plug in all terms and set to 0:
$$
\frac{\partial}{\partial{\hat{\beta}}}RSS(\hat{\beta}) =
\color{grey}{0}
\color{black}{-}
\color{red}{2\mathbf{X}^Ty}
\color{black}{+}
\color{green}{2\mathbf{X}^T\mathbf{X}\hat{\beta}}
\color{black}{\ = 0}
$$</p>
<p>Rearranging, we get:
$$
\color{red}{\mathbf{X}^Ty}
\color{black}{=}
\color{green}{\mathbf{X}^T\mathbf{X}\hat{\beta}}
$$</p>
<p>And if $ \mathbf{X}^T\mathbf{X} $ is nonsingular, i.e. it has an inverse, we can solve directly for $ \hat{\beta} $ using the following:
$$ \hat{\beta} = (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^Ty$$</p>
<p>This is called the normal equation. It is helpful when the inverse $ (\mathbf{X}^T\mathbf{X})^{-1}\ $ is not too computationally intensive.</p>
<h2 id="maximum-likelihood-estimate-formulation">Maximum Likelihood Estimate Formulation</h2>
<p>From <a href="https://en.wikipedia.org/wiki/Maximum_likelihood_estimation">wikipedia</a>: &ldquo;Maximum Likelihood Estimation is a method of estimating the parameters of a probability distribution by maximizing a likelihood function, so that under the assumed statistical model the observed data is most probable&rdquo;. Some probabilistic process generated some data - which process parameters would have made our dataset the most likely? Expressed as conditional probability distributions, for some data, which parameters maximize $ P(data | parameters) $?</p>
<p>As stated before, the population regression line is:</p>
<p>$$ Y = \mathbf{X}\beta + \epsilon $$</p>
<p>where $ \mathbf{X}\beta $ is constant. Gaussian linear regression assumes that $ \epsilon $ is normally distributed with mean 0 and variance $ \sigma^2 $, $ \epsilon \sim N(0, \sigma^2) $. Examining the distribution of $ Y$ under this assumption, noting that $E[X + Y] = E[X] + E[Y] $ regardless of independence:</p>
<p>$$ E[Y] = E[\mathbf{X}\beta + \epsilon] $$
$$ = E[\mathbf{X}\beta] + E[\epsilon] $$
$$ = E[\mathbf{X}\beta] + 0 $$
$$ = \mathbf{X}\beta $$</p>
<p>And for variance, since $Var(X) = E[X^2] - (E[X])^2 $ (from <a href="https://en.wikipedia.org/wiki/Variance">here</a>) and $ Cov(X,Y) = E[XY] - E[X]E[Y] $ (from <a href="https://en.wikipedia.org/wiki/Covariance">here</a>), then:</p>
<p>$$ Var(A + B) = E[(A + B)^2] - (E[A + B])^2 $$
$$ = E[A^2 + 2AB + B^2] - (E[A] + E[B])^2 $$
$$ = E[A^2] + E[2AB] + E[B^2] $$
$$ - E[A]^2 - 2E[A]E[B] - E[B]^2 $$
$$ = E[A^2] - E[A]^2 $$
$$  + E[B^2] - E[B]^2 $$
$$  + 2(E[AB] - E[A]E[B]) $$
$$ = Var(A) + Var(B) + 2Cov(A,B) $$</p>
<p>Substituting $A = \mathbf{X}\beta $, $B = \epsilon$ and recognizing Covariance as a measure of correlation between variables - positive if both tend to increase together, negative when one tends to increase while the other decreases - we can say:</p>
<p>$$ Var(\mathbf{X}\beta + \epsilon) = Var(\mathbf{X}\beta) + Var(\epsilon) + 2Cov(\mathbf{X}\beta, \epsilon) $$
$$ = 0 + \sigma^2 + 2Cov(\mathbf{X}\beta, \epsilon) $$</p>
<p>Due to the assumptions of linear regression outlined above (exogeneity and homoscedasticity), the correlation between $\mathbf{X}\beta$ and $\epsilon$ is zero. We conclude:</p>
<p>$$ Var(Y) = \sigma^2 $$</p>
<p>so:</p>
<p>$$ Y \sim N(\mathbf{X}\beta, \sigma^2) $$</p>
<p>With this in mind, we begin the actual MLE derivation. Given that the PDF of the gaussian distribution is:</p>
<p>$$ P(x|\mu_x, \sigma_x^2) = \frac{1}{\sigma_x\sqrt{2\pi}} \exp{\left\lbrace-\frac{1}{2\sigma_x^2}(x-\mu_x)\right\rbrace} $$</p>
<p>we can say the conditional probability of some datum $ y_i $ based on our parameters is:</p>
<p>$$ P(y_i|x_i, \beta, \sigma^2) = \frac{1}{\sigma\sqrt{2\pi}} \exp{\left\lbrace-\frac{1}{2\sigma^2}(y_i - x_i\beta)^2\right\rbrace} $$</p>
<p>where we somehow go through $ i = 0,1,2,&hellip;m $ data points and obtain the joint probability of all of them. Assuming our data are i.i.d. (independently and identically distributed, i.e. sampled from the same probability distribution and independent of one another) - we can say the joint probability of all the data is a product of individual probabilities:</p>
<p>$$ P(\{y_i\}_{i=0}^m | \{x_i\}_{i=0}^m, \beta, \sigma^2) = $$
$$  \prod_{i=0}^m{\frac{1}{\sigma\sqrt{2\pi}} \exp{\left\lbrace-\frac{1}{2\sigma^2}(y_i - x_i\beta)^2\right\rbrace}} $$</p>
<p>In matrix form:
$$ P(Y|X,\beta,\sigma^2) = $$
$$ \left(\frac{1}{\sigma\sqrt{2\pi}}\right)^m \exp{\left\lbrace-\frac{1}{2\sigma^2}(y-\mathbf{X}\beta)^T(y-\mathbf{X}\beta)\right\rbrace} $$</p>
<p>With MLE, we often take the log, as maximizing a summation is easier than maximizing a product due to ease of derivation. Note that the parameters that maximize some random variable X are the same that maximize log(X), so we&rsquo;re ok doing this.
$$ \ln(P(Y|X,\beta,\sigma^2)) = $$
$$  \color{red}{ m\ln\left(\frac{1}{\sigma\sqrt{2\pi}}\right) }
\color{black}{ - }
\color{green}{ \frac{1}{2\sigma^2}(y-\mathbf{X}\beta)^T(y-\mathbf{X}\beta) }
$$</p>
<p>Note that the red term doesn&rsquo;t have a $ \beta $ and is therefore irrelevant while we try to find the $ \beta $ that maximimizes $ \ln(P(Y|X,\beta,\sigma^2)) $. In order to do this, we must actually MINIMIZE the green term, as it&rsquo;s negative! So let&rsquo;s see&hellip;we want to minimize&hellip;
$$ \frac{1}{2\sigma^2}(y-\mathbf{X}\beta)^T(y-\mathbf{X}\beta) $$</p>
<p>Sound familiar? It&rsquo;s the same problem as the &ldquo;Normal Equation for Estimated Weights&rdquo; section before! This will again result in the normal equation:
$$ \beta^{MLE} = (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^Ty$$</p>
<p>Now note, we ignored the red term above to get the $ \beta^{MLE} $, but we can also maximize the joint probability distribution with respect to $ \sigma^2 $ in the MLE formulation. We derive and set to zero to find the maximum:
$$ \frac{\partial}{\partial{\sigma}}
\left( \color{red}{ m\ln\left(\frac{1}{\sigma\sqrt{2\pi}}\right) } \right)
\color{black}{ - \frac{\partial}{\partial{\sigma}} }
\left( \color{green}{ \frac{1}{2\sigma^2}(y-\mathbf{X}\beta)^T(y-\mathbf{X}\beta)) } \right)
$$
$$ = \color{red}{ -m (\sigma\sqrt{2\pi}) \left(\frac{1}{\sigma^2\sqrt{2\pi}}\right) }
\color{black}{ + }
\color{green}{ \frac{1}{\sigma^3}(y-\mathbf{X}\beta)^T(y-\mathbf{X}\beta)) }
$$
$$ = \color{red}{ -m }
\color{black}{ + }
\color{green}{ \frac{1}{\sigma^2}(y-\mathbf{X}\beta)^T(y-\mathbf{X}\beta)) }
\color{black}{ = 0 }
$$</p>
<p>Rerranging, we get the MLE estimate for the variance of $ \epsilon $:
$$ \sigma^{2MLE} = \frac{1}{m}(y-\mathbf{X}\beta)^T(y-\mathbf{X}\beta) $$</p>
<p>And note how this (satisfyingly) resembles the population variance formula for a normal distribution:
$$ \sigma^2_x = E[(x - \mu_x)^2] $$</p>
<h2 id="confidence-in-estimated-parameters">Confidence in Estimated Parameters</h2>
<p>(Insert justification for searching for unbiased sigma^2 and variance of beta in the first place here)</p>
<p>Unfortunately, the MLE variance is actually a biased estimate of the population variance, i.e. $ E[\sigma^{2MLE}] \neq \sigma^2 $! For a discussion around this and empirical proof, see <a href="https://robinovitch61.github.io/posts/statistical-bias/">here</a>.</p>

<figure>
  <img style="max-width: 95%;" src="/img/biased_mle_error_var.png" alt="biased_mle_error_var.png" />
  <figcaption class="figcaption-text">MLE (left) mean estimate differs from the true value</figcaption>
  
</figure>
<p>The unbiased formula for estimated error variance comes from <a href="http://lukesonnet.com/teaching/inference/200d_standard_errors.pdf">here</a> and the rigorous proof <a href="https://stats.stackexchange.com/questions/20227/why-is-rss-distributed-chi-square-times-n-p">here</a>:
$$ \sigma^{2OLS} = \frac{1}{m-n} (y-\mathbf{X}\beta)^T (y-\mathbf{X}\beta) $$</p>
<p>(Variance of beta here)</p>
<p><em>To be included sometime in the future: standard errors, confidence bounds, hypothesis tests, numerical methods.</em></p>
<!-- Sources -->
<h2 id="references">References:</h2>
<ul>
<li><a href="https://eli.thegreenplace.net/2014/derivation-of-the-normal-equation-for-linear-regression/">https://eli.thegreenplace.net/2014/derivation-of-the-normal-equation-for-linear-regression/</a></li>
<li><a href="https://eli.thegreenplace.net/2015/the-normal-equation-and-matrix-calculus/#id3">https://eli.thegreenplace.net/2015/the-normal-equation-and-matrix-calculus/#id3</a></li>
<li><a href="https://math.stackexchange.com/questions/2753210/when-can-we-say-that-a-mathrm-t-b-b-mathrm-t-a">https://math.stackexchange.com/questions/2753210/when-can-we-say-that-a-mathrm-t-b-b-mathrm-t-a</a></li>
<li><a href="https://ayearofai.com/rohan-3-deriving-the-normal-equation-using-matrix-calculus-1a1b16f65dda">https://ayearofai.com/rohan-3-deriving-the-normal-equation-using-matrix-calculus-1a1b16f65dda</a></li>
<li><a href="https://en.wikipedia.org/wiki/Transpose#Properties">https://en.wikipedia.org/wiki/Transpose#Properties</a></li>
<li><a href="https://github.com/robinovitch61/coursera_MachineLearning_AndrewNg">https://github.com/robinovitch61/coursera_MachineLearning_AndrewNg</a></li>
<li><a href="https://www.cs.princeton.edu/courses/archive/fall18/cos324/files/mle-regression.pdf">https://www.cs.princeton.edu/courses/archive/fall18/cos324/files/mle-regression.pdf</a></li>
<li><a href="https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf">https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf</a></li>
<li><a href="http://3.droppdf.com/files/pjxkI/regression-analysis-by-example-5th-edition.pdf">http://3.droppdf.com/files/pjxkI/regression-analysis-by-example-5th-edition.pdf</a></li>
<li><a href="https://stats.stackexchange.com/questions/263324/%5Bhow-can-the-regression-error-term-ever-be-correlated-with-the-explanatory-variab%5D(how-can-the-regression-error-term-ever-be-correlated-with-the-explanatory-variab)">https://stats.stackexchange.com/questions/263324/[how-can-the-regression-error-term-ever-be-correlated-with-the-explanatory-variab](how-can-the-regression-error-term-ever-be-correlated-with-the-explanatory-variab)</a></li>
<li><a href="http://www.lithoguru.com/scientist/statistics/Lecture21.pdf">http://www.lithoguru.com/scientist/statistics/Lecture21.pdf</a></li>
<li><a href="https://stats.stackexchange.com/questions/349244/beginner-q-residual-sum-squared-rss-and-r2">https://stats.stackexchange.com/questions/349244/beginner-q-residual-sum-squared-rss-and-r2</a></li>
<li><a href="https://en.wikipedia.org/wiki/Maximum_likelihood_estimation">https://en.wikipedia.org/wiki/Maximum_likelihood_estimation</a></li>
<li><a href="https://www.youtube.com/watch?v=ulZW99jsMXY">https://www.youtube.com/watch?v=ulZW99jsMXY</a></li>
<li><a href="https://medium.com/@komotlogelwa/r-squared-and-life-3cb220d5a03f">https://medium.com/@komotlogelwa/r-squared-and-life-3cb220d5a03f</a></li>
<li><a href="https://www.usna.edu/Users/math/dphillip/sa421.s16/variance-of-a-sum.pdf">https://www.usna.edu/Users/math/dphillip/sa421.s16/variance-of-a-sum.pdf</a></li>
<li><a href="https://en.wikipedia.org/wiki/Variance">https://en.wikipedia.org/wiki/Variance</a></li>
<li><a href="https://en.wikipedia.org/wiki/Covariance">https://en.wikipedia.org/wiki/Covariance</a></li>
<li><a href="https://brilliant.org/wiki/linearity-of-expectation/">https://brilliant.org/wiki/linearity-of-expectation/</a></li>
<li><a href="http://lukesonnet.com/teaching/inference/200d_standard_errors.pdf">http://lukesonnet.com/teaching/inference/200d_standard_errors.pdf</a></li>
<li><a href="https://web.stanford.edu/~mrosenfe/soc_meth_proj3/matrix_OLS_NYU_notes.pdf">https://web.stanford.edu/~mrosenfe/soc_meth_proj3/matrix_OLS_NYU_notes.pdf</a></li>
</ul>

  </article>

  <article>
    <h2>jupyter with docker</h2>
    <p>I love <a href="https://www.docker.com">docker</a>. It makes environments:</p>
<ul>
<li>portable across any machine</li>
<li>subject to version control (retraceable and recoverable history)</li>
<li>easily deployed and discarded with simple commands</li>
</ul>
<p>Jupyter is a great tool that allows for the creation of nice looking documents consisting of ordered code chunks with inline output. It is a fantastic way to get started with programming, clearly step through your workflow, and/or create stories and presentations out of your work. It runs in the browser after you install it on your machine (i.e. you access it with a URL in your web browser).</p>

<figure>
  <img style="max-width: 95%;" src="/img/jupyterlab_ex.png" alt="jupyterlab_ex.png" />
  
  <figcaption class="figcaption-text">Source: <a href="https://jupyterlab.readthedocs.io/en/stable/">https://jupyterlab.readthedocs.io/en/stable/</a></figcaption>
</figure>
<p>This walkthrough will get you set up with a jupyter lab (or jupyter notebooks classic) environment that is fully customizable, isolated, version controllable and portable using docker.</p>
<p>If you don&rsquo;t want to use docker, you can always <a href="https://jupyter.org/install">install jupyter the classic way</a>. I like docker for the reasons outlined above (isolation, portability, version controllable, easily deployed/discarded).</p>
<h2 id="1-install-docker">1: Install Docker</h2>
<p>To install Docker, you&rsquo;ll have to follow the instructions below:</p>
<ul>
<li><a href="https://docs.docker.com/docker-for-mac/install/">Install docker for Mac</a></li>
<li><a href="https://docs.docker.com/install/linux/docker-ce/ubuntu/">Install docker for Ubuntu</a></li>
<li><a href="https://docs.docker.com/docker-for-windows/install/">Install docker for Windows 10</a></li>
</ul>
<p>Once installed, you&rsquo;ll want to open a terminal (or cmd.exe for windows) and enter <code>docker</code>. You should see a long list of options and commands and not something like <code>command 'docker' not found</code>. If you see the long list, you&rsquo;re ready to proceed!</p>
<p>Docker uses the language of images (<em>recipes for containers</em>) and containers (<em>instances of images</em>). In this specific case, things like <code>jupyter/minimal-notebook</code> and <code>jupyter/pyspark-notebook</code> are <strong>images</strong> while our <strong>container</strong> will just be called <code>juptyer</code>.</p>
<p>I like to think of a container as an isolated environment that my app/service (in this case, jupyter) will run in, almost entirely isolated from the rest of my host machine.</p>

<figure>
  <img style="max-width: 95%;" src="/img/docker-infrastructure.png" alt="docker-infrastructure.png" />
  
  <figcaption class="figcaption-text">Source: <a href="https://www.docker.com/resources/what-container">https://www.docker.com/resources/what-container</a></figcaption>
</figure>
<h2 id="2-clone-the-docker-stacks-repo">2: Clone the <code>docker-stacks</code> Repo</h2>
<p>You&rsquo;ll need git for this step, which you can <a href="https://git-scm.com/book/en/v2/Getting-Started-Installing-Git">install here</a> if you don&rsquo;t already have it. You can check by typing <code>git</code> in to the terminal and seeing if the command is found.</p>
<p>What you&rsquo;ll clone is <a href="https://github.com/jupyter/docker-stacks">Project Jupyter&rsquo;s docker-stacks</a>. You can also <a href="https://help.github.com/en/articles/fork-a-repo">fork this</a> repo and clone your fork. We&rsquo;ll go through adding a couple files to this repo that will make deployment and customizing things easy. You can use <a href="https://github.com/robinovitch61/docker-stacks">my fork</a> or this tutorial as a reference. To clone the <code>docker-stacks</code> repo, run the following command in terminal/command prompt:</p>
<div class="highlight"><div style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4">
<table style="border-spacing:0;padding:0;margin:0;border:0;width:auto;overflow:auto;display:block;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#408080;font-style:italic"># run in the desired directory, e.g. ~/projects </span>
git clone https://github.com/jupyter/docker-stacks.git
</code></pre></td></tr></table>
</div>
</div><p>The <code>docker-stacks</code> directory we just cloned contains a lot of stuff. Open the folder in your favorite editor or just look at the repo <a href="https://github.com/jupyter/docker-stacks">here</a>. We&rsquo;re going to look at just a few bits of a few important files. First, the <code>Dockerfile</code> in the <code>/base-notebook</code> directory:</p>
<div class="highlight"><div style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4">
<table style="border-spacing:0;padding:0;margin:0;border:0;width:auto;overflow:auto;display:block;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">12
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">13
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">14
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">15
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">16
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">17
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#408080;font-style:italic">############################################</span>
<span style="color:#408080;font-style:italic">## docker-stacks/base-notebook/Dockerfile ##</span>
<span style="color:#408080;font-style:italic">############################################</span>

<span style="color:#408080;font-style:italic"># Copyright (c) Jupyter Development Team.</span>
<span style="color:#408080;font-style:italic"># Distributed under the terms of the Modified BSD License.</span>

<span style="color:#408080;font-style:italic"># Ubuntu 18.04 (bionic) from 2019-06-12</span>
<span style="color:#408080;font-style:italic"># https://github.com/tianon/docker-brew-ubuntu-core/commit/3c462555392cb188830b7c91e29311b5fad90cfe</span>
ARG <span style="color:#19177c">BASE_CONTAINER</span><span style="color:#666">=</span>ubuntu:bionic-20190612@sha256:9b1702dcfe32c873a770a32cfd306dd7fc1c4fd134adfb783db68defc8894b3c
FROM <span style="color:#19177c">$BASE_CONTAINER</span>

LABEL <span style="color:#19177c">maintainer</span><span style="color:#666">=</span><span style="color:#ba2121">&#34;Jupyter Project&#34;</span>
ARG <span style="color:#19177c">NB_USER</span><span style="color:#666">=</span><span style="color:#ba2121">&#34;jovyan&#34;</span>
ARG <span style="color:#19177c">NB_UID</span><span style="color:#666">=</span><span style="color:#ba2121">&#34;1000&#34;</span>
ARG <span style="color:#19177c">NB_GID</span><span style="color:#666">=</span><span style="color:#ba2121">&#34;100&#34;</span>
<span style="color:#408080;font-style:italic"># ...</span>
</code></pre></td></tr></table>
</div>
</div><p>This <code>Dockerfile</code> specifies that all jupyter containers will derive from a Linux Ubuntu &ldquo;bionic&rdquo; OS (<code>BASE_CONTAINER=ubuntu...</code>).</p>
<p>If you examine the <code>Dockerfile</code>s for other directories, you&rsquo;ll see there&rsquo;s a chain of derivations that end in the base-notebook:</p>
<div class="highlight"><div style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4">
<table style="border-spacing:0;padding:0;margin:0;border:0;width:auto;overflow:auto;display:block;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#408080;font-style:italic"># minimal-notebook/Dockerfile, top section:</span>
ARG <span style="color:#19177c">BASE_CONTAINER</span><span style="color:#666">=</span>jupyter/base-notebook

<span style="color:#408080;font-style:italic"># scipy-notebook/Dockerfile, top section:</span>
ARG <span style="color:#19177c">BASE_CONTAINER</span><span style="color:#666">=</span>jupyter/minimal-notebook

<span style="color:#408080;font-style:italic"># datascience-notebook/Dockerfile, top section:</span>
ARG <span style="color:#19177c">BASE_CONTAINER</span><span style="color:#666">=</span>jupyter/scipy-notebook

<span style="color:#408080;font-style:italic"># pyspark-notebook/Dockerfile, top section:</span>
ARG <span style="color:#19177c">BASE_CONTAINER</span><span style="color:#666">=</span>jupyter/scipy-notebook
</code></pre></td></tr></table>
</div>
</div><p>The docker-stacks team has structured the project so you can specify how complicated of a setup you want out of the box based on what image you select. The <code>jupyter/minimal-notebook</code> image will contain the simplest installation you need for jupyter to run at all. The <code>jupyter/pyspark-notebook</code> image will come with a lot more, like a spark installation in the jupyter container file system (likely overkill unless you&rsquo;re using pyspark!). Higher up images in the inheritance tree will include everything below them (e.g. pyspark-notebook contains everything from scipy-notebook, minimal-notebook, and base-notebook).</p>
<h3 id="custom-file-1-docker-compose">Custom File 1: Docker Compose</h3>
<p>The first file we add to the <code>docker-stacks</code> base repo is <code>docker-compose.yml</code>. The <code>docker-compose</code> tool is often used for orchestrating many containers (e.g. &ldquo;bring up an nginx container and a jupyter container together so nginx can act as a reverse proxy and make the jupyter URL more user-friendly&rdquo;), but here we only have one container (<code>jupyter</code>). I like to use <code>docker-compose.yml</code> as a way of version controlling a <code>docker run</code> command, which is typically used to bring single containers up. All <code>docker-run</code> commands can be translated into <code>docker-compose</code> files and vice versa.</p>
<p>You&rsquo;ll create a <code>docker-stacks/docker-compose.yml</code> file that looks something like this (totally up to you to customize!):</p>
<div class="highlight"><div style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4">
<table style="border-spacing:0;padding:0;margin:0;border:0;width:auto;overflow:auto;display:block;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">12
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">13
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">14
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">15
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">16
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">17
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">18
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">19
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">20
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">21
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">22
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">23
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">24
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#408080;font-style:italic">############################################</span>
<span style="color:#408080;font-style:italic">## docker-stacks/docker-compose.yml ##</span>
<span style="color:#408080;font-style:italic">############################################</span>
version: <span style="color:#ba2121">&#39;3&#39;</span>

services:

    jupyter:
    container_name: jupyter
    image: jupyter/minimal-notebook <span style="color:#408080;font-style:italic"># always get `latest` image on docker hub</span>
    <span style="color:#408080;font-style:italic"># image: jupyter/pyspark-notebook:1386e2046833 # tagged/pinned image example. check for updates here: https://hub.docker.com/r/jupyter/pyspark-notebook/tags.</span>
    volumes:
        - ~/:/home/jovyan/work/
        - ./setup_envs.sh:/usr/local/bin/before-notebook.d/setup_envs.sh
    ports:
        - 9999:9999
    restart: unless-stopped
    environment:
        JUPYTER_ENABLE_LAB: <span style="color:#ba2121">&#39;yes&#39;</span>
    command: &gt;
        sh -c <span style="color:#ba2121">&#34;bash start-notebook.sh\
</span><span style="color:#ba2121">                --NotebookApp.token=</span><span style="color:#19177c">$JUPYTER_PASSWORD</span><span style="color:#ba2121">\
</span><span style="color:#ba2121">                --NotebookApp.notebook_dir=/home/jovyan/work/\
</span><span style="color:#ba2121">                --port 9999&#34;</span>
</code></pre></td></tr></table>
</div>
</div><p>Meaning of this witchcraft:</p>
<ul>
<li><code>version '3'</code>: docker-compose specifies versions. They have slightly different syntax between them</li>
<li><code>services</code>: our list of services here is just one service called jupyter that will create a container called jupyter</li>
<li><code>image</code>: here is where you call out which image you want to use, as discussed above</li>
<li><code>volumes</code>: this is how you break isolation between the container and host environment, linking directories and/or files from the host to the container filesystems. This is important because otherwise when you destroy your container, your files would be lost with it. Syntax * <code>host_path:container_path</code>. Changes made in either host or container will reflect in the other</li>
<li><code>ports</code>: jupyter will run on a port in the container. <code>ports</code> binds the host port to the container port. Syntax is <code>host_port:container_port</code></li>
<li><code>restart: unless-stopped</code>: if your container fails, it will auto-restart unless you stop it with e.g* <code>docker-compose down</code></li>
<li><code>environment</code>: specify environment variables. <code>JUPYTER_ENABLE_LAB</code> enables jupyter lab by default</li>
<li><code>command</code>: this is the final command docker will run in the container upon build. <code>NotebookApp.token</code> password-protects your jupyter instance. The port here should also match the container port * <code>ports</code> above</li>
</ul>
<h3 id="custom-file-2-setup-script">Custom File 2: Setup Script</h3>
<p>You may have noticed that the second volume specified in the <code>docker-compose.yml</code> file is called <code>./setup_envs.sh</code>. This is a custom startup script that I use to do the following:</p>
<ul>
<li>Create environment variables that are accessible in jupyter</li>
<li>Add jupyterlab extensions</li>
<li>Create custom conda environments and link them to jupyter kernels</li>
<li>Run other useful commands in the container upon startup (e.g. configure git, install <code>vim</code> or <code>svn</code>, etc.)</li>
</ul>
<p>You&rsquo;ll create a <code>docker-stacks/setup_envs.sh</code> file that looks something like this (totally up to you to customize!):</p>
<div class="highlight"><div style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4">
<table style="border-spacing:0;padding:0;margin:0;border:0;width:auto;overflow:auto;display:block;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">12
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">13
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">14
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">15
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">16
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">17
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">18
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">19
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">20
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">21
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">22
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">23
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">24
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">25
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">26
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">27
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">28
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">29
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">30
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">31
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">32
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">33
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">34
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">35
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">36
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">37
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">38
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">39
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">40
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">41
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">42
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">43
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">44
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">45
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">46
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">47
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">48
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">49
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">50
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">51
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">52
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">53
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">54
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">55
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">56
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">57
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">58
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">59
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">60
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">61
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">62
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">63
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">64
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#408080;font-style:italic">############################################</span>
<span style="color:#408080;font-style:italic">## example docker-stacks/setup_envs.sh ##</span>
<span style="color:#408080;font-style:italic">############################################</span>
<span style="color:#008000">echo</span> <span style="color:#ba2121">&#34;&#34;</span>
<span style="color:#008000">echo</span> <span style="color:#ba2121">&#34;Creating useful environment variables...&#34;</span>
<span style="color:#008000">export</span> <span style="color:#19177c">MYVAR</span><span style="color:#666">=</span><span style="color:#19177c">$MYVAR</span>
<span style="color:#008000">export</span> <span style="color:#19177c">MYVAR2</span><span style="color:#666">=</span><span style="color:#19177c">$MYVAR2</span>
<span style="color:#008000">echo</span> <span style="color:#ba2121">&#34;Finished creating useful environment variables!&#34;</span>

<span style="color:#008000">echo</span> <span style="color:#ba2121">&#34;&#34;</span>
<span style="color:#008000">echo</span> <span style="color:#ba2121">&#34;Adding jupyterlab extensions...&#34;</span>
rm -rf /home/jovyan/work/extensions
mkdir /home/jovyan/work/extensions

<span style="color:#408080;font-style:italic"># Great extension by calpoly for better markdown formatting in cells: https://github.com/jupytercalpoly/jupyterlab-richtext-mode</span>
git clone https://github.com/jupytercalpoly/jupyterlab-richtext-mode.git /home/jovyan/work/extensions/jupyter-scribe <span style="color:#666">&amp;&amp;</span> <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span><span style="color:#008000">cd</span> /home/jovyan/work/extensions/jupyter-scribe <span style="color:#666">&amp;&amp;</span> <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>npm install <span style="color:#666">&amp;&amp;</span> <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>npm run build <span style="color:#666">&amp;&amp;</span> <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>jupyter labextension link .

<span style="color:#408080;font-style:italic"># Rebuild to get extensions to work</span>
jlpm build
jupyter lab build
<span style="color:#008000">echo</span> <span style="color:#ba2121">&#34;Added jupyterlab extensions!&#34;</span>

<span style="color:#008000">echo</span> <span style="color:#ba2121">&#34;&#34;</span>
<span style="color:#008000">echo</span> <span style="color:#ba2121">&#34;Creating kernel py37...&#34;</span>
conda create -n py37 <span style="color:#19177c">python</span><span style="color:#666">=</span>3.7.4 pip -y
<span style="color:#008000">echo</span> <span style="color:#ba2121">&#34;Finished creating kernel py37!&#34;</span>

<span style="color:#408080;font-style:italic"># Install packages with pip, otherwise conda&#39;s &#34;auto-checking compatibility&#34; algo freaks out (with certain versions of conda)</span>
<span style="color:#008000">echo</span> <span style="color:#ba2121">&#34;&#34;</span>
<span style="color:#008000">echo</span> <span style="color:#ba2121">&#34;Installing packages in py37...&#34;</span>
<span style="color:#008000">source</span> activate py37 <span style="color:#666">&amp;&amp;</span> <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>pip install ipykernel <span style="color:#666">&amp;&amp;</span> <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>pip install pandas <span style="color:#666">&amp;&amp;</span> <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>pip install numpy <span style="color:#666">&amp;&amp;</span> <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>pip install scipy <span style="color:#666">&amp;&amp;</span> <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>pip install matplotlib <span style="color:#666">&amp;&amp;</span> <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>pip install seaborn <span style="color:#666">&amp;&amp;</span> <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span><span style="color:#008000">echo</span> <span style="color:#ba2121">&#34;Finished installing packages in py37!&#34;</span>

<span style="color:#008000">echo</span> <span style="color:#ba2121">&#34;&#34;</span>
<span style="color:#008000">echo</span> <span style="color:#ba2121">&#34;Connecting env to kernel...&#34;</span>
<span style="color:#008000">source</span> activate py37 <span style="color:#666">&amp;&amp;</span> python -m ipykernel install --user --name py37 --display-name <span style="color:#ba2121">&#34;py37&#34;</span>
<span style="color:#008000">source</span> activate py37 <span style="color:#408080;font-style:italic"># must activate/deactivate once to have changes displayed</span>
<span style="color:#008000">source</span> activate base
<span style="color:#008000">echo</span> <span style="color:#ba2121">&#34;Connected env to kernel!&#34;</span>

cat &lt;&lt; <span style="color:#ba2121">&#34;EOF&#34;</span>

WELCOME!

     ‚ñà‚ñà‚ïó‚ñà‚ñà‚ïó   ‚ñà‚ñà‚ïó‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó ‚ñà‚ñà‚ïó   ‚ñà‚ñà‚ïó‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó
     ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïó‚ïö‚ñà‚ñà‚ïó ‚ñà‚ñà‚ïî‚ïù‚ïö‚ïê‚ïê‚ñà‚ñà‚ïî‚ïê‚ïê‚ïù‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ïê‚ïù‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïó
     ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïî‚ïù ‚ïö‚ñà‚ñà‚ñà‚ñà‚ïî‚ïù    ‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïî‚ïù
‚ñà‚ñà   ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ïù   ‚ïö‚ñà‚ñà‚ïî‚ïù     ‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïî‚ïê‚ïê‚ïù  ‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïó
‚ïö‚ñà‚ñà‚ñà‚ñà‚ñà‚ïî‚ïù‚ïö‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïî‚ïù‚ñà‚ñà‚ïë        ‚ñà‚ñà‚ïë      ‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ïë  ‚ñà‚ñà‚ïë
 ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïù  ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù ‚ïö‚ïê‚ïù        ‚ïö‚ïê‚ïù      ‚ïö‚ïê‚ïù   ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù‚ïö‚ïê‚ïù  ‚ïö‚ïê‚ïù

Take it from here...

EOF
</code></pre></td></tr></table>
</div>
</div><p><em>Dirty details on how this runs just by placing it in the container:</em> the <code>start-notebook.sh</code> script run by the <code>docker-compose</code> command will subsequently run <code>start.sh</code>. Both these shell scripts are in the <code>base-notebook</code> directory. The <code>start.sh</code> script has &ldquo;hooks&rdquo; for running scripts that are in the <code>/usr/local/bin/before-notebook.d</code>. <strong>All this to say that <code>setup_envs.sh</code> will be run as part of your jupyter setup</strong> because we put it in the <code>/usr/local/bin/before-notebook.d</code> directory in the jupyter container!</p>
<p>Note that you could totally exclude the <code>setup_envs.sh</code> script and volume and your deployment would be fully functioning. I like to use this method as a way of customizing my jupyter setup in a version-controlled manner. Rather than installing a new python package in my base environment or installing a system package every time manually and trying to remember what I did when I port to a new machine, I&rsquo;ll edit the <code>setup_envs.sh</code> script, commit and push to my fork, then redeploy on whatever machine I want and get the exact same setup.</p>
<h2 id="3-bring-up-the-jupyter-container">3: Bring Up the Jupyter Container</h2>
<p>If you&rsquo;ve gotten this far, you&rsquo;re basically done! The one thing we&rsquo;re missing is to set a password/token. In terminal, run this command with your custom password. If you don&rsquo;t do this, <code>docker-compose</code> will warn you at the next step.</p>
<div class="highlight"><div style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4">
<table style="border-spacing:0;padding:0;margin:0;border:0;width:auto;overflow:auto;display:block;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#008000">export</span> <span style="color:#19177c">JUPYTER_PASSWORD</span><span style="color:#666">=</span><span style="color:#19177c">$MY_CUSTOM_PASSWORD</span>
</code></pre></td></tr></table>
</div>
</div><p>You can now navigate to the <code>docker-stacks</code> directory in a terminal and run:</p>
<div class="highlight"><div style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4">
<table style="border-spacing:0;padding:0;margin:0;border:0;width:auto;overflow:auto;display:block;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">docker-compose up -d
</code></pre></td></tr></table>
</div>
</div><p>The <code>-d</code> flag stands for &ldquo;detached&rdquo;, and ensures the <code>jupyter</code> container will stay running even if you exit the terminal.</p>
<p>You can now run:</p>
<div class="highlight"><div style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4">
<table style="border-spacing:0;padding:0;margin:0;border:0;width:auto;overflow:auto;display:block;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">docker logs -f jupyter
</code></pre></td></tr></table>
</div>
</div><p>The <code>-f</code> flag standing for &ldquo;follow&rdquo;. Watch the progress in your terminal as your jupyter container is instantiated and your <code>setup_envs.sh</code> script is run (if you made one).</p>
<p>Once you see something like the following:</p>
<div class="highlight"><div style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4">
<table style="border-spacing:0;padding:0;margin:0;border:0;width:auto;overflow:auto;display:block;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">4
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#666">[</span>I 16:19:04.574 LabApp<span style="color:#666">]</span> The Jupyter Notebook is running at:
<span style="color:#666">[</span>I 16:19:04.574 LabApp<span style="color:#666">]</span> http://3204808557f5:9999/?token<span style="color:#666">=</span>...
<span style="color:#666">[</span>I 16:19:04.575 LabApp<span style="color:#666">]</span>  or http://127.0.0.1:9999/?token<span style="color:#666">=</span>...
<span style="color:#666">[</span>I 16:19:04.575 LabApp<span style="color:#666">]</span> Use Control-C to stop this server and shut down all kernels <span style="color:#666">(</span>twice to skip confirmation<span style="color:#666">)</span>.
</code></pre></td></tr></table>
</div>
</div><p>You&rsquo;re ready to go! Navigate to <a href="http://127.0.0.1:9999">http://127.0.0.1:9999</a> (or whatever port you specified if different than <code>9999</code>) where you should see a page that prompts you for your password (set with the <code>JUPYTER_PASSWORD</code> environment variable earlier).</p>
<p>You have a fully functioning and replicable jupyter environment, portable to any machine that runs docker! Remember that even though your container has access to your host file system through the volume we created, your file system isn&rsquo;t (and shouldn&rsquo;t be) version controlled with the <code>docker-stacks</code> directory. Version control your projects separately.</p>
<h3 id="further-thoughts">Further thoughts</h3>
<ul>
<li>Another approach to version controlled customization could be to modify the Dockerfiles themselves. I prefer the <code>setup_envs.sh</code> script approach as rebuilding the images is time and disk space intensive. Modifying Dockerfiles, though, is a totally valid approach.</li>
<li>If you get stuck on anything in this process or see that I&rsquo;ve been wrong/unclear about something, please open an issue on the source of this website <a href="https://github.com/robinovitch61/the-leo-zone">here.</li>
</ul>

  </article>


    </div>
    
    <footer>
  <div class="container">
  </div>
</footer>

  </body>
</html>